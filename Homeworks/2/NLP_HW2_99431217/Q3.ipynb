{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8Wio74Smvmm"
      },
      "source": [
        "1.Submit a Google Colab notebook containing your completed code and experimentation results.\n",
        "\n",
        "2.Include comments and explanations in your code to help understand the implemented logic.\n",
        "\n",
        "**Additional Notes:**\n",
        "*   Ensure that the notebook runs successfully in Google Colab.\n",
        "*   Document any issues encountered during experimentation and how you addressed them.\n",
        "\n",
        "**Grading:**\n",
        "*   Each task will be graded out of the specified points.\n",
        "*   Points will be awarded for correctness, clarity of code, thorough experimentation, and insightful analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oCb3cVwzrfbb"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# SOURCE_DIR = '/content/Q3_data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HtCgCrUVtU_J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uEa_gZkKmvmv"
      },
      "outputs": [],
      "source": [
        "def delete_hashtag_usernames(text):\n",
        "  try:\n",
        "    result = []\n",
        "    for word in text.split():\n",
        "      if word[0] not in ['@', '#']:\n",
        "        result.append(word)\n",
        "    return ' '.join(result)\n",
        "  except:\n",
        "    return ''\n",
        "\n",
        "def delete_url(text):\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "  return text\n",
        "\n",
        "def delete_ex(text):\n",
        "  text = re.sub(r'\\u200c', '', text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz4cWWgA4xBF"
      },
      "source": [
        "# 0. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0FjQ9_ZMkve",
        "outputId": "2f8239b7-d28f-44c7-ab60-b5ac04881347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: json-lines in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from json-lines) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install json-lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Uf7K92-olSfe"
      },
      "outputs": [],
      "source": [
        "import json_lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bG5awXKo40HS"
      },
      "outputs": [],
      "source": [
        "# 1. extract all tweets from file and save them in memory\n",
        "# 2. remove urls, hashtags and usernames. use the prepared functions\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv('Q3_data.csv')\n",
        "# print(data.columns)\n",
        "\n",
        "# Apply preprocessing functions to the tweet data\n",
        "data['Text'] = data['Text'].apply(delete_hashtag_usernames)\n",
        "data['Text'] = data['Text'].apply(delete_url)\n",
        "data['Text'] = data['Text'].apply(delete_ex)\n",
        "\n",
        "# Print the preprocessed tweet data\n",
        "# print(data['Text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cncc0Uxvmvmy"
      },
      "source": [
        "# 1. Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZjVk9qgmvmz"
      },
      "source": [
        "## Cosine Similarity\n",
        "\n",
        "To measure the similarity between two words, you need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows:\n",
        "\n",
        "$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta)Â \\tag{1}$$\n",
        "\n",
        "* $u \\cdot v$ is the dot product (or inner product) of two vectors\n",
        "* $||u||_2$ is the norm (or length) of the vector $u$\n",
        "* $\\theta$ is the angle between $u$ and $v$.\n",
        "* The cosine similarity depends on the angle between $u$ and $v$.\n",
        "    * If $u$ and $v$ are very similar, their cosine similarity will be close to 1.\n",
        "    * If they are dissimilar, the cosine similarity will take a smaller value.\n",
        "\n",
        "<img src=\"images/cosine_sim.png\" style=\"width:800px;height:250px;\">\n",
        "<caption><center><font color='purple'><b>Figure 1</b>: The cosine of the angle between two vectors is a measure of their similarity.</font></center></caption>\n",
        "\n",
        "Implement the function `cosine_similarity()` to evaluate the similarity between word vectors.\n",
        "\n",
        "**Reminder**: The norm of $u$ is defined as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dfiK-Lw7mvm0"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(u, v):\n",
        "    \"\"\"\n",
        "    Cosine similarity reflects the degree of similarity between u and v\n",
        "\n",
        "    Arguments:\n",
        "        u -- a word vector of shape (n,)\n",
        "        v -- a word vector of shape (n,)\n",
        "\n",
        "    Returns:\n",
        "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
        "    \"\"\"\n",
        "\n",
        "    dot_product = np.dot(u, v)\n",
        "    norm_u = np.linalg.norm(u) ##########\n",
        "    norm_v = np.linalg.norm(v) ##########\n",
        "    cosine_similarity = dot_product / (norm_u * norm_v)\n",
        "\n",
        "    return cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLDGkeBRmvm0"
      },
      "source": [
        "## find k nearest neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EaO0XQvKmvm1"
      },
      "outputs": [],
      "source": [
        "def find_k_nearest_neighbors(word, embedding_dict, k):\n",
        "  \"\"\"\n",
        "    implement a function to return the nearest words to an specific word based on the given dictionary\n",
        "\n",
        "    Arguments:\n",
        "        word           -- a word, string\n",
        "        embedding_dict -- dictionary that maps words to their corresponding vectors\n",
        "        k              -- the number of word that should be returned\n",
        "\n",
        "    Returns:\n",
        "        a list of size k consisting of the k most similar words to the given word\n",
        "\n",
        "    Note: use the cosine_similarity function that you have implemented to calculate the similarity between words\n",
        "    \"\"\"\n",
        "  # Ensure the word is in the embedding dictionary\n",
        "  if word not in embedding_dict:\n",
        "      return []\n",
        "\n",
        "  # Get the embedding for the word\n",
        "  word_embedding = embedding_dict[word]\n",
        "\n",
        "  # Calculate cosine similarity with all other words\n",
        "  similarities = {}\n",
        "  for other_word, other_embedding in embedding_dict.items():\n",
        "      if other_word != word:\n",
        "          sim = cosine_similarity(word_embedding, other_embedding)\n",
        "          similarities[other_word] = sim\n",
        "\n",
        "  # Sort by similarity\n",
        "  sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "  # Extract the top k words\n",
        "  neighbors = [word for word, _ in sorted_similarities[:k]]\n",
        "\n",
        "  return neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqr5DLDYuKd-"
      },
      "source": [
        "# 2. One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. find one hot encoding of each word\n",
        "\n",
        "words = data['Text'].str.split().tolist()\n",
        "words = [word for sublist in words for word in sublist]\n",
        "# print(words)\n",
        "\n",
        "# Reshape the words to be a column vector\n",
        "words_array = np.array(words).reshape(-1, 1)\n",
        "\n",
        "# Create the encoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Fit and transform the words to one-hot encoded vectors\n",
        "one_hot_encoded = encoder.fit_transform(words_array)\n",
        "\n",
        "# # Create a DataFrame to view the one-hot encoded words\n",
        "# one_hot_df = pd.DataFrame(one_hot_encoded, index=words, columns=encoder.get_feature_names_out())\n",
        "\n",
        "# # Display the one-hot encoded DataFrame\n",
        "# print(one_hot_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIZzhK_El9Al",
        "outputId": "22f45d13-309a-4f6a-8677-7574661e49ca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPqc0I0yuNlI",
        "outputId": "4d290d1b-526a-4441-834e-55f6f2827fae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ø¨ÙØ´ÛÙ', 'ØªØ§', 'Ø´ÙØ¯', 'ÙÙØ´', 'ÙØ§Ù', 'ÙØ§', 'ÙÙ', 'ÙØ±Ø¯Ø§', 'Ø´Ø¯Ù', 'Ø§ÛÙ']\n"
          ]
        }
      ],
      "source": [
        "# 2. find 10 nearest words from \"Ø¢Ø²Ø§Ø¯Û\"\n",
        "embedding_dict = {word: encoding for word, encoding in zip(words, one_hot_encoded)}\n",
        "\n",
        "word = \"Ø¢Ø²Ø§Ø¯Û\"\n",
        "k = 10\n",
        "\n",
        "nearest_words = find_k_nearest_neighbors(word, embedding_dict, k)\n",
        "\n",
        "print(nearest_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Just testing\n",
        "count = 0\n",
        "for value in embedding_dict['Ø¢Ø²Ø§Ø¯Û']:\n",
        "  if value != 0.0:\n",
        "    print(count, value)\n",
        "  count = count + 1\n",
        "\n",
        "count = 0\n",
        "for value in embedding_dict['Ø¨ÙØ§Ø±Ù']:\n",
        "  if value != 0.0:\n",
        "    print(count, value)\n",
        "  count = count + 1\n",
        "\n",
        "count = 0\n",
        "for value in embedding_dict['Ú©Ø§ÙÙ¾ÛÙØªØ±']:\n",
        "  if value != 0.0:\n",
        "    print(count, value)\n",
        "  count = count + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pasR5WChpOKV",
        "outputId": "15f51dd7-c7cb-4eb4-fb5f-ab48654ef36e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1805 1.0\n",
            "7146 1.0\n",
            "28946 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FayPXc1mvm2"
      },
      "source": [
        "##### Describe advantages and disadvantages of one-hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9tAFn4brKrB"
      },
      "source": [
        "**Advantages:**\n",
        "\n",
        "1. **Simplicity:** One-hot encoding is a straightforward and simple method to represent categorical variables. It involves creating a binary vector where each element corresponds to a unique category, making it easy to understand and implement.\n",
        "\n",
        "2. **Retains categorical information:** One-hot encoding preserves the categorical nature of the variable. Each category is represented by a separate binary variable, allowing models to capture relationships and patterns specific to each category.\n",
        "\n",
        "3. **Compatibility with machine learning algorithms:** Many machine learning algorithms require numerical inputs. One-hot encoding converts categorical variables into a numeric format that can be readily used by these algorithms.\n",
        "\n",
        "4. **Avoids ordinality assumption:** One-hot encoding treats all categories as independent and does not impose any ordinal relationship between categories. This is useful when there is no inherent order or hierarchy among the categories.\n",
        "\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "1. **Dimensionality:** One-hot encoding expands the dimensionality of the feature space. If a categorical variable has a large number of unique categories, the resulting one-hot encoded representation can lead to a high-dimensional feature space, which may impact computational efficiency and model complexity.\n",
        "\n",
        "2. **Curse of dimensionality:** The increase in dimensionality due to one-hot encoding can lead to the curse of dimensionality. This refers to the problem where the number of features becomes large relative to the number of observations, which can result in sparse data, increased model complexity, and overfitting.\n",
        "\n",
        "3. **Redundancy:** One-hot encoding can introduce redundancy in the data representation. Since each category is represented by a separate binary variable, there is a perfect correlation between these variables. This redundancy can lead to multicollinearity issues in some models.\n",
        "\n",
        "4. **Handling new categories:** One-hot encoding requires defining the set of categories in advance. If new categories appear during testing or deployment, the one-hot encoding scheme may not handle them properly. This can be particularly problematic in real-world scenarios where new categories may emerge over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHeSYFUKw5gw"
      },
      "source": [
        "# 3. TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tweets = data['Text']\n",
        "print(Tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WUmrM9dsOMS",
        "outputId": "2450112e-a735-4560-c681-aa08c34ae55e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                  Ø¨ÙØ´ÛÙ ØªØ§ Ø´ÙØ¯ ÙÙØ´ ÙØ§Ù ÙØ§ ÙÙØ´ ÙÙ ÙØ±Ø¯Ø§ Ø´Ø¯Ù\n",
            "1        Ø§ÛÙ Ú¯ÙØ²Ù Ø±Ù Ú©Û Ú¯Ø±Ø¯Ù ÙÛÚ¯ÛØ±ÙØØ Ø¯ÚØ§Ø± Ø²ÙØ§Ù Ø¹ÙÙ Ø´Ø¯Ù...\n",
            "2                                   Ø¨Ø±Ø§Û Ø§ÛØ±Ø§ÙØ Ø¨Ø±Ø§Û ÙÙØ³Ø§.\n",
            "3                                          ÙØ±Ú¯ Ø¨Ø± Ø¯ÛÚ©ØªØ§ØªÙØ±\n",
            "4                               ÙØ°Ø§Ø±ÛÙ Ø®ÙÙØ´ÙÙ Ù¾Ø§ÛÙØ§Ù Ø´Ù...\n",
            "                               ...                        \n",
            "19995                                     Ø¨Ø±Ø§Û Ø§ÛØ±Ø§Ù Ø¨Ø§ÙÙ \n",
            "19996        Ø§Ø² Ø¨Ø³ Ø­Ø§Ø¬ Ø®Ø§ÙÙ Ø¯Ø±Ø§Ø² ÙØ´Ø¯Ù ÙØ§Ø³Ø´ Ø¹ÙØ¯Ù Ø¯Ø±Ø§Ø² Ø¯Ø§Ø±Ùð\n",
            "19997    Ø¨Ù Ø§ÙØªØ®Ø§Ø± Ø§Ø² Ø¨ÛÙ Ø±ÙØªÙ Ø¬ÙÙÙØ±Û Ø§Ø³ÙØ§ÙÛðââï¸ðââï¸ðââ...\n",
            "19998                                          Ù¾ÙØ¬Ø§Ù Ù Ø´ÛØ´\n",
            "19999    Ø¯Ø± ÙØ­ÛØ· Ø·ÙÙØ§ÙØ²Ø§Û ÙØ§ÙØ±Ø§ÙÙ Ø¯Ø± Ø¬ÙÚ¯ Ø§Ø³Øª ÙØ§Ø®Ø¯Ø§Û Ø§Ø³Øª...\n",
            "Name: Text, Length: 20000, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. find the TF-IDF of all tweets.\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a document-term matrix\n",
        "vectorizer = TfidfVectorizer()\n",
        "document_term_matrix = vectorizer.fit_transform(Tweets)\n",
        "\n",
        "# Calculate the TF-IDF values\n",
        "tfidf_values = document_term_matrix.toarray()\n",
        "\n",
        "# Normalize the TF-IDF values (optional)\n",
        "normalized_tfidf = tfidf_values / np.linalg.norm(tfidf_values, axis=1, keepdims=True)\n",
        "\n",
        "# Print the TF-IDF values for the first 10 tweets\n",
        "for i in range(10):\n",
        "    print(\"Tweet:\", Tweets[i])\n",
        "    print(\"TF-IDF:\", normalized_tfidf[i])\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yFr8FMAsbpJ",
        "outputId": "587de178-036c-4638-d8f2-ad51937de868"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet: Ø¨ÙØ´ÛÙ ØªØ§ Ø´ÙØ¯ ÙÙØ´ ÙØ§Ù ÙØ§ ÙÙØ´ ÙÙ ÙØ±Ø¯Ø§ Ø´Ø¯Ù\n",
            "TF-IDF: [0. 0. 0. ... 0. 0. 0.]\n",
            "\n",
            "Tweet: Ø§ÛÙ Ú¯ÙØ²Ù Ø±Ù Ú©Û Ú¯Ø±Ø¯Ù ÙÛÚ¯ÛØ±ÙØØ Ø¯ÚØ§Ø± Ø²ÙØ§Ù Ø¹ÙÙ Ø´Ø¯Ù Ø§Ø² Ø¨Ø³ Ù¾Ø§Û ÙÙØ¨Ø± Ø¯Ø³ØªÙØ§Ù Ú©Ø´Û Ú©Ø±Ø¯Ù.\n",
            "TF-IDF: [0. 0. 0. ... 0. 0. 0.]\n",
            "\n",
            "Tweet: Ø¨Ø±Ø§Û Ø§ÛØ±Ø§ÙØ Ø¨Ø±Ø§Û ÙÙØ³Ø§.\n",
            "TF-IDF: [0. 0. 0. ... 0. 0. 0.]\n",
            "\n",
            "Tweet: ÙØ±Ú¯ Ø¨Ø± Ø¯ÛÚ©ØªØ§ØªÙØ±\n",
            "TF-IDF: [0. 0. 0. ... 0. 0. 0.]\n",
            "\n",
            "Tweet: ÙØ°Ø§Ø±ÛÙ Ø®ÙÙØ´ÙÙ Ù¾Ø§ÛÙØ§Ù Ø´Ù...\n",
            "TF-IDF: [0. 0. 0. ... 0. 0. 0.]\n",
            "\n",
            "Tweet: ÙØ§Ø¨ÙØª Ø§ÙØªØ®Ø§Ø± ÙÛÚ©ÙÛÙ ÙØ¨Ø§Øª Ø¨Ø§Ø¹Ø« Ø´Ø¯Û Ú©Ù Ø¯ÙÛØ§ ÙØ§Ø±Ù Ø¨Ø¨ÛÙÙ\n",
            "TF-IDF: [0. 0. 0. ... 0. 0. 0.]\n",
            "\n",
            "Tweet: Ø¨Ø±Ø§Û Ø§ÙØ³Ø§ÙØ§Û Ø®ÙØ´Ú¯ÙÙÙÙ\n",
            "TF-IDF: [0. 0. 0. ... 0. 0. 0.]\n",
            "\n",
            "Tweet: ÙØ§Ø±Øº Ø§Ø² ÙØ± Ø¨Ø§ÙØ±Û ÙØªØ­Ø¯ Ø´ÙÛÙ.\n",
            "TF-IDF: [0. 0. 0. ... 0. 0. 0.]\n",
            "\n",
            "Tweet: Ø§ÛÙÙØ§ Ø¹Ø¬Ø¨ ÙÙØ¬ÙØ¯Ø§Øª Ù¾Ø³ØªÛ ÙØ³ØªÙð¥ºð¥ºð¥ºØ§ÙÙÛ Ø¨Ú¯Ø±Ø¯ÙØ ÙÙ Ø®ÙØ¯Ù Ø¨Ø§Ø±Ø¯Ø§Ø±Ù Ù Ø­ØªÛ ØªÙØªØ¸Ø§ÙØ±Ø§Øª ÙØ³Ø§ÙÙØª Ø§ÙÛØ² Ø®Ø§Ø±Ø¬ Ø§ÛØ±Ø§Ù Ø§Ø³ØªØ±Ø³ Ø¯Ø§Ø´ØªÙ Ø§Ø¯Ù ÙØ§ ÙØ§ Ø®ÙØ¯ Ø§Ú¯Ø§Ù Ø¨ÙÙ Ø¶Ø±Ø¨Ù Ø¨Ø²ÙÙØØ¨ÙÛØ±Ù Ø¨Ø±Ø§Û Ø¯Ù Ø§ÙÙ Ø²Ù Ú©Ù ÚÙ Ú©Ø´ÛØ¯Ù...ÙØ±Ú¯ Ø¨Ø± Ø¯ÛÚ©ØªØ§ØªÙØ±\n",
            "TF-IDF: [0. 0. 0. ... 0. 0. 0.]\n",
            "\n",
            "Tweet: Ú©ØµØ®ÙØ§ ÚØ±Ø§ Û´ ØªØ§ÙÙØ­Ø´Ø´ ÙÙÛØ¯ÙØ\n",
            "TF-IDF: [0. 0. 0. ... 0. 0. 0.]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-8ff8337f2c8e>:13: RuntimeWarning: divide by zero encountered in divide\n",
            "  normalized_tfidf = tfidf_values / np.linalg.norm(tfidf_values, axis=1, keepdims=True)\n",
            "<ipython-input-15-8ff8337f2c8e>:13: RuntimeWarning: invalid value encountered in divide\n",
            "  normalized_tfidf = tfidf_values / np.linalg.norm(tfidf_values, axis=1, keepdims=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "SJMju0Tiw9YA"
      },
      "outputs": [],
      "source": [
        "# 2. choose one tweets randomly.\n",
        "# 3. find 10 nearest tweets from chosen tweet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqNCL4kFmvm3"
      },
      "source": [
        "##### Describe advantages and disadvantages of TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GHvpTAZu7ZU"
      },
      "source": [
        "Advatages:\n",
        "\n",
        "\n",
        "Disadvantages:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INM6vtm2zqJs"
      },
      "source": [
        "# 4. Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCnxqaVY2zCc"
      },
      "outputs": [],
      "source": [
        "# 1. train a word2vec model base on all tweets\n",
        "# 2. find 10 nearest words from \"Ø¢Ø²Ø§Ø¯Û\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37oMUvZcmvm4"
      },
      "source": [
        "##### Describe advantages and disadvantages of Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv39LXf0wkjd"
      },
      "source": [
        "Advantages:\n",
        "\n",
        "\n",
        "Disadvantages:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSdlWMl64aPN"
      },
      "source": [
        "# 5. Contextualized embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgVAEQhyOPxg"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfKEqNml6eEB"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc8hBCnn4cV_"
      },
      "outputs": [],
      "source": [
        "# 1. fine-tune the model base on all tweets\n",
        "# 2. find 10 nearest words from \"Ø¢Ø²Ø§Ø¯Û\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBUkqdX0mvm6"
      },
      "source": [
        "##### Describe advantages and disadvantages of Contextualized embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al-4HT8KxTwd"
      },
      "source": [
        "Advantages:\n",
        "\n",
        "\n",
        "Disadvantages:\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}