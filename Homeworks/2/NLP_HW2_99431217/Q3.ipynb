{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8Wio74Smvmm"
      },
      "source": [
        "1.Submit a Google Colab notebook containing your completed code and experimentation results.\n",
        "\n",
        "2.Include comments and explanations in your code to help understand the implemented logic.\n",
        "\n",
        "**Additional Notes:**\n",
        "*   Ensure that the notebook runs successfully in Google Colab.\n",
        "*   Document any issues encountered during experimentation and how you addressed them.\n",
        "\n",
        "**Grading:**\n",
        "*   Each task will be graded out of the specified points.\n",
        "*   Points will be awarded for correctness, clarity of code, thorough experimentation, and insightful analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oCb3cVwzrfbb"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# SOURCE_DIR = '/content/Q3_data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HtCgCrUVtU_J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uEa_gZkKmvmv"
      },
      "outputs": [],
      "source": [
        "def delete_hashtag_usernames(text):\n",
        "  try:\n",
        "    result = []\n",
        "    for word in text.split():\n",
        "      if word[0] not in ['@', '#']:\n",
        "        result.append(word)\n",
        "    return ' '.join(result)\n",
        "  except:\n",
        "    return ''\n",
        "\n",
        "def delete_url(text):\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "  return text\n",
        "\n",
        "def delete_ex(text):\n",
        "  text = re.sub(r'\\u200c', '', text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz4cWWgA4xBF"
      },
      "source": [
        "# 0. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0FjQ9_ZMkve",
        "outputId": "edd0d9a0-e2e3-4d3e-9b61-744747ea6b5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: json-lines in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from json-lines) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install json-lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Uf7K92-olSfe"
      },
      "outputs": [],
      "source": [
        "import json_lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bG5awXKo40HS"
      },
      "outputs": [],
      "source": [
        "# 1. extract all tweets from file and save them in memory\n",
        "# 2. remove urls, hashtags and usernames. use the prepared functions\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv('Q3_data.csv')\n",
        "# print(data.columns)\n",
        "\n",
        "PureText_data = data['PureText']\n",
        "\n",
        "# Apply preprocessing functions to the tweet data\n",
        "PureText_data = PureText_data.apply(delete_hashtag_usernames)\n",
        "PureText_data = PureText_data.apply(delete_url)\n",
        "PureText_data = PureText_data.apply(delete_ex)\n",
        "\n",
        "# Print the preprocessed tweet data\n",
        "# print(data['Text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cncc0Uxvmvmy"
      },
      "source": [
        "# 1. Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZjVk9qgmvmz"
      },
      "source": [
        "## Cosine Similarity\n",
        "\n",
        "To measure the similarity between two words, you need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows:\n",
        "\n",
        "$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta)Â \\tag{1}$$\n",
        "\n",
        "* $u \\cdot v$ is the dot product (or inner product) of two vectors\n",
        "* $||u||_2$ is the norm (or length) of the vector $u$\n",
        "* $\\theta$ is the angle between $u$ and $v$.\n",
        "* The cosine similarity depends on the angle between $u$ and $v$.\n",
        "    * If $u$ and $v$ are very similar, their cosine similarity will be close to 1.\n",
        "    * If they are dissimilar, the cosine similarity will take a smaller value.\n",
        "\n",
        "<img src=\"images/cosine_sim.png\" style=\"width:800px;height:250px;\">\n",
        "<caption><center><font color='purple'><b>Figure 1</b>: The cosine of the angle between two vectors is a measure of their similarity.</font></center></caption>\n",
        "\n",
        "Implement the function `cosine_similarity()` to evaluate the similarity between word vectors.\n",
        "\n",
        "**Reminder**: The norm of $u$ is defined as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dfiK-Lw7mvm0"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(u, v):\n",
        "    \"\"\"\n",
        "    Cosine similarity reflects the degree of similarity between u and v\n",
        "\n",
        "    Arguments:\n",
        "        u -- a word vector of shape (n,)\n",
        "        v -- a word vector of shape (n,)\n",
        "\n",
        "    Returns:\n",
        "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
        "    \"\"\"\n",
        "\n",
        "    dot_product = np.dot(u, v)\n",
        "    norm_u = np.sqrt(np.sum(u**2))\n",
        "    norm_v = np.sqrt(np.sum(v**2))\n",
        "    cosine_similarity = dot_product / (norm_u * norm_v)\n",
        "\n",
        "    return cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLDGkeBRmvm0"
      },
      "source": [
        "## find k nearest neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EaO0XQvKmvm1"
      },
      "outputs": [],
      "source": [
        "def find_k_nearest_neighbors(word, embedding_dict, k):\n",
        "  \"\"\"\n",
        "    implement a function to return the nearest words to an specific word based on the given dictionary\n",
        "\n",
        "    Arguments:\n",
        "        word           -- a word, string\n",
        "        embedding_dict -- dictionary that maps words to their corresponding vectors\n",
        "        k              -- the number of word that should be returned\n",
        "\n",
        "    Returns:\n",
        "        a list of size k consisting of the k most similar words to the given word\n",
        "\n",
        "    Note: use the cosine_similarity function that you have implemented to calculate the similarity between words\n",
        "    \"\"\"\n",
        "  # Ensure the word is in the embedding dictionary\n",
        "  if word not in embedding_dict:\n",
        "      return []\n",
        "\n",
        "  # Get the embedding for the word\n",
        "  word_embedding = embedding_dict[word]\n",
        "\n",
        "  # Calculate cosine similarity with all other words\n",
        "  similarities = {}\n",
        "  for other_word, other_embedding in embedding_dict.items():\n",
        "      # print(\"Other word is: \", other_word)\n",
        "      # print(\"other_embedding is: \", other_embedding)\n",
        "      if other_word != word:\n",
        "          sim = cosine_similarity(word_embedding, other_embedding)\n",
        "          similarities[other_word] = sim\n",
        "          # if sim != 0.0:\n",
        "            # print(\"sim is: \", sim)\n",
        "  # print (\"Similarities is: \", similarities)\n",
        "  # Sort by similarity\n",
        "  sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "  # Extract the top k words\n",
        "  # neighbors = [word for word, _ in sorted_similarities[:k]]\n",
        "  neighbors = sorted_similarities[:k]\n",
        "\n",
        "  return neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqr5DLDYuKd-"
      },
      "source": [
        "# 2. One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. find one hot encoding of each word\n",
        "\n",
        "words = PureText_data.str.split().tolist()\n",
        "words = [word for sublist in words for word in sublist]\n",
        "# print(words[1003])\n",
        "\n",
        "# Reshape the words to be a column vector\n",
        "words_array = np.array(words).reshape(-1, 1)\n",
        "# print(words_array)\n",
        "\n",
        "# Create the encoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Fit and transform the words to one-hot encoded vectors\n",
        "one_hot_encoded = encoder.fit_transform(words_array)\n",
        "\n",
        "# # Create a DataFrame to view the one-hot encoded words\n",
        "# one_hot_df = pd.DataFrame(one_hot_encoded, index=words, columns=encoder.get_feature_names_out())\n",
        "\n",
        "# # Display the one-hot encoded DataFrame\n",
        "# print(one_hot_df)"
      ],
      "metadata": {
        "id": "qIZzhK_El9Al",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e198908-3b2c-40d9-bd8e-1dff0adaea89"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPqc0I0yuNlI",
        "outputId": "def75f87-9de2-4529-9ebe-663cfdf44f32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Ø¨Ù†Ø´ÛŒÙ†', 0.0), ('ØªØ§', 0.0), ('Ø´ÙˆØ¯', 0.0), ('Ù†Ù‚Ø´', 0.0), ('ÙØ§Ù„', 0.0), ('Ù…Ø§', 0.0), ('Ù‡Ù…', 0.0), ('ÙØ±Ø¯Ø§', 0.0), ('Ø´Ø¯Ù†', 0.0), ('Ø§ÛŒÙ†', 0.0)]\n",
            "[('Ø¨Ù†Ø´ÛŒÙ†', 0.0), ('ØªØ§', 0.0), ('Ø´ÙˆØ¯', 0.0), ('Ù†Ù‚Ø´', 0.0), ('ÙØ§Ù„', 0.0), ('Ù…Ø§', 0.0), ('Ù‡Ù…', 0.0), ('ÙØ±Ø¯Ø§', 0.0), ('Ø´Ø¯Ù†', 0.0), ('Ø§ÛŒÙ†', 0.0)]\n"
          ]
        }
      ],
      "source": [
        "# 2. find 10 nearest words from \"Ø¢Ø²Ø§Ø¯ÛŒ\"\n",
        "embedding_dict = {word: encoding for word, encoding in zip(words, one_hot_encoded)}\n",
        "\n",
        "word = \"Ø¢Ø²Ø§Ø¯ÛŒ\"\n",
        "k = 10\n",
        "nearest_words = find_k_nearest_neighbors(word, embedding_dict, k)\n",
        "print(nearest_words)\n",
        "\n",
        "word = \"Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±\"\n",
        "nearest_words = find_k_nearest_neighbors(word, embedding_dict, k)\n",
        "print(nearest_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each and every vector in the one hot encoding is orthogonal to each other. So the cosine similarity as well as distance between any two vectors are same. Thus it holds no relationship among them.\n",
        "That is why the nearest words found are the same. The cosine similarity of each pair of words equals 0."
      ],
      "metadata": {
        "id": "_yinx6QW8EfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just testing\n",
        "count = 0\n",
        "for value in embedding_dict['Ø¢Ø²Ø§Ø¯ÛŒ']:\n",
        "  if value != 0.0:\n",
        "    print(count, value)\n",
        "  count = count + 1\n",
        "\n",
        "count = 0\n",
        "for value in embedding_dict['Ø¨Ù‡Ø§Ø±Ù‡']:\n",
        "  if value != 0.0:\n",
        "    print(count, value)\n",
        "  count = count + 1\n",
        "\n",
        "count = 0\n",
        "for value in embedding_dict['Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±']:\n",
        "  if value != 0.0:\n",
        "    print(count, value)\n",
        "  count = count + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pasR5WChpOKV",
        "outputId": "96735a66-18ee-4f1d-83c7-078dcb0d1b39"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1805 1.0\n",
            "7146 1.0\n",
            "28946 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FayPXc1mvm2"
      },
      "source": [
        "##### Describe advantages and disadvantages of one-hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9tAFn4brKrB"
      },
      "source": [
        "**Advantages:**\n",
        "\n",
        "1. **Simplicity:** One-hot encoding is a straightforward and simple method to represent categorical variables. It involves creating a binary vector where each element corresponds to a unique category, making it easy to understand and implement.\n",
        "\n",
        "2. **Retains categorical information:** One-hot encoding preserves the categorical nature of the variable. Each category is represented by a separate binary variable, allowing models to capture relationships and patterns specific to each category.\n",
        "\n",
        "3. **Compatibility with machine learning algorithms:** Many machine learning algorithms require numerical inputs. One-hot encoding converts categorical variables into a numeric format that can be readily used by these algorithms.\n",
        "\n",
        "4. **Avoids ordinality assumption:** One-hot encoding treats all categories as independent and does not impose any ordinal relationship between categories. This is useful when there is no inherent order or hierarchy among the categories.\n",
        "\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "1. **Dimensionality:** One-hot encoding expands the dimensionality of the feature space. If a categorical variable has a large number of unique categories, the resulting one-hot encoded representation can lead to a high-dimensional feature space, which may impact computational efficiency and model complexity.\n",
        "\n",
        "2. **Curse of dimensionality:** The increase in dimensionality due to one-hot encoding can lead to the curse of dimensionality. This refers to the problem where the number of features becomes large relative to the number of observations, which can result in sparse data, increased model complexity, and overfitting.\n",
        "\n",
        "3. **Redundancy:** One-hot encoding can introduce redundancy in the data representation. Since each category is represented by a separate binary variable, there is a perfect correlation between these variables. This redundancy can lead to multicollinearity issues in some models.\n",
        "\n",
        "4. **Handling new categories:** One-hot encoding requires defining the set of categories in advance. If new categories appear during testing or deployment, the one-hot encoding scheme may not handle them properly. This can be particularly problematic in real-world scenarios where new categories may emerge over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHeSYFUKw5gw"
      },
      "source": [
        "# 3. TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tweets = data['Text']\n",
        "print(Tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WUmrM9dsOMS",
        "outputId": "97841db6-ffa6-4895-8027-21801d7205d3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        Ø¨Ù†Ø´ÛŒÙ† ØªØ§ Ø´ÙˆØ¯ Ù†Ù‚Ø´ ÙØ§Ù„ Ù…Ø§ \\nÙ†Ù‚Ø´ Ù‡Ù…â€Œ ÙØ±Ø¯Ø§ Ø´Ø¯Ù†\\n#Ù…...\n",
            "1        @Tanasoli_Return @dr_moosavi Ø§ÛŒÙ† Ú¯ÙˆØ²Ùˆ Ø±Ùˆ Ú©ÛŒ Ú¯Ø±...\n",
            "2        @ghazaleghaffary Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ØŒ Ø¨Ø±Ø§ÛŒ Ù…Ù‡Ø³Ø§.\\n#OpIr...\n",
            "3        @_hidden_ocean Ù…Ø±Ú¯ Ø¨Ø± Ø¯ÛŒÚ©ØªØ§ØªÙˆØ± \\n#OpIran \\n#Ma...\n",
            "4        Ù†Ø°Ø§Ø±ÛŒÙ… Ø®ÙˆÙ†Ø´ÙˆÙ† Ù¾Ø§ÛŒÙ…Ø§Ù„ Ø´Ù‡.â€Œâ€Œ.â€Œâ€Œ.\\n#Mahsa_Amini #...\n",
            "                               ...                        \n",
            "19995    Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ±Ø§Ù† Ø¨Ø§Ù†Ùˆ #Mahsa_Amini      #MahsaAmini ...\n",
            "19996    @MohammadTehra16 @mimpedram Ø§Ø² Ø¨Ø³ Ø­Ø§Ø¬ Ø®Ø§Ù†Ù… Ø¯Ø±Ø§...\n",
            "19997    Ø¨Ù‡ Ø§ÙØªØ®Ø§Ø± Ø§Ø² Ø¨ÛŒÙ† Ø±ÙØªÙ† Ø¬Ù…Ù‡ÙˆØ±ÛŒ Ø§Ø³Ù„Ø§Ù…ÛŒğŸ™†â€â™‚ï¸ğŸ™†â€â™‚ï¸ğŸ™†â€â™‚...\n",
            "19998    Ù¾Ù†Ø¬Ø§Ù‡ Ùˆ Ø´ÛŒØ´ \\n\\n#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \\n#Mahsa_Amini \\n#...\n",
            "19999    Ø¯Ø± Ù…Ø­ÛŒØ· Ø·ÙˆÙØ§Ù†â€ŒØ²Ø§ÛŒ Ù…Ø§Ù‡Ø±Ø§Ù†Ù‡ Ø¯Ø± Ø¬Ù†Ú¯ Ø§Ø³Øª\\nÙ†Ø§Ø®Ø¯Ø§ÛŒ Ø§...\n",
            "Name: Text, Length: 20000, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. find the TF-IDF of all tweets.\n",
        "########## import and preprocess (PureText_data)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# # Create a document-term matrix\n",
        "# vectorizer = TfidfVectorizer()\n",
        "# document_term_matrix = vectorizer.fit_transform(Tweets)\n",
        "\n",
        "# # Calculate the TF-IDF values\n",
        "# tfidf_values = document_term_matrix.toarray()\n",
        "\n",
        "# # Normalize the TF-IDF values\n",
        "# normalized_tfidf = tfidf_values / np.linalg.norm(tfidf_values, axis=1, keepdims=True)\n",
        "\n",
        "# # Print the TF-IDF values for the first 10 tweets\n",
        "# for i in range(10):\n",
        "#     print(\"Tweet:\", Tweets[i])\n",
        "#     print(\"TF-IDF:\", normalized_tfidf[i])\n",
        "#     print()\n",
        "\n",
        "# Create a TfidfVectorizer object and fit it to the preprocessed corpus\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(words)\n",
        "\n",
        "# Transform the preprocessed corpus into a TF-IDF matrix\n",
        "tf_idf_matrix = vectorizer.transform(words)\n",
        "\n",
        "# Get list of feature names that correspond to the columns in the TF-IDF matrix\n",
        "print(\"Feature Names:\\n\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print the resulting matrix\n",
        "print(\"TF-IDF Matrix:\\n\", tf_idf_matrix.toarray())\n",
        "# for i in tf_idf_matrix.toarray():\n",
        "#   for j in i:\n",
        "#     if (j != 0):\n",
        "#       print (j)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yFr8FMAsbpJ",
        "outputId": "9f6dff8d-f4bd-4171-c864-40e2c06093a3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names:\n",
            " ['00' '0020115687' '00971562643674' ... 'Û¹Û¸' 'Û¹Û¹' 'ïºØ³Øª']\n",
            "TF-IDF Matrix:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. choose one tweets randomly.\n",
        "import random\n",
        "chosen_tweet = random.choice(Tweets)\n",
        "print(\"Chosen Tweet:\", chosen_tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUJVtLSeE_yH",
        "outputId": "509daa62-6bde-4b44-9114-de1f397cbc7b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen Tweet: @AkkasNabashi ØµØ¯ Ø¯Ø± ØµØ¯ Ø¯Ø±Ø³ØªÙ‡.\n",
            "#Mahsa_Amini #Ù‡Ø³ØªÛŒ_Ù†Ø§Ø±ÙˆÛŒÛŒ #Ù…Ø¬ÛŒØ¯Ø±Ø¶Ø§_Ø±Ù‡Ù†ÙˆØ±Ø¯ #Ø³Ø§Ù…Ø§Ù†_ÛŒØ§Ø³ÛŒÙ†\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SJMju0Tiw9YA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970724f9-b1c8-4e9e-f6c8-0389b524409b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-fb2fe9b1de3f>:16: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  cosine_similarity = dot_product / (norm_u * norm_v)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 Nearest Tweets:\n",
            "========================\n",
            "Ø¨Ø±Ø§ÛŒ Ø²Ù† Ø²Ù†Ø¯Ú¯ÛŒ Ø¢Ø²Ø§Ø¯ÛŒ\n",
            "#Mahsa_Amini\n",
            "========================\n",
            "@HichkasOfficial @Itpry_ Ø¬Ù…Ù‡ÙˆØ±ÛŒ Ø§Ø¬Ø¨Ø§Ø±ÛŒ Ù‡Ø§\n",
            "Ø³Ø±Ø¨Ø§Ø²ÛŒ Ù‡Ø§\n",
            "Ø­Ø¬Ø§Ø¨ Ù‡Ø§\n",
            "ØµØ¯Ø§ Ù‡Ø§\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ #MahsaAmini #Mahsa_Amini #OpIran\n",
            "========================\n",
            "Ø¨Ø±Ø®ÛŒ ØªØµØ§ÙˆÛŒØ± Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø´Ù…Ø§ Ø±Ø§ Ø¨Ù‡ Ø±Ø§Ù‡ Ø±Ø§ÛŒØª Ù‡Ø¯Ø§ÛŒØª Ú©Ù†Ø¯\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ #Ù…Ù‡Ø³Ø§Ø§Ù…ÛŒÙ†ÛŒ #Ø§Ø¹ØªØµØ§Ø¨Ø§Øª_Ø³Ø±Ø§Ø³Ø±ÛŒ #mahsa_amini #mahsaamini #opiran\n",
            "========================\n",
            "Ø®ÛŒÙ„ÛŒ Ø²ÙˆØ¯ Ù…ÛŒØ±Ø³Ù‡ Ú©Ù‡ Ø¨Ú¯ÛŒÙ…:\n",
            "Ø³Ø®Øª Ø¨ÙˆØ¯ ÙˆÙ„ÛŒ Ø¨Ø§Ù„Ø§Ø®Ø±Ù‡ ØªÙˆÙ†Ø³ØªÛŒÙ… Ùˆ Ø§Ø² Ù¾Ø³Ø´ Ø¨Ø± Ø§ÙˆÙ…Ø¯ÛŒÙ… :)!\n",
            "#Mahsa_Amini\n",
            "========================\n",
            "ØµÙ…ÛŒÙ…Ø§Ù†Ù‡ Ø§Ø² Ø§Ø±ØªØ´ Ø¯Ø± Ø®ÙˆØ§Ø³Øª Ú©Ù…Ú© Ø¯Ø§Ø±ÛŒÙ…ØŒ\n",
            "Ø§Ø±ØªØ´ÛŒ Ù‡Ø§ÛŒ ØºÛŒÙˆØ±ØŒ Ù…Ø±Ø¯Ù… Ú†Ù†Ø¯ ÙˆÙ‚ØªÙ‡ Ú©Ù‡ Ù…Ù†ØªØ¸Ø± Ø´Ù…Ø§ Ù‡Ø³ØªÙ†...\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ\n",
            "#Mahsa_Amini \n",
            "#Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡_Ø´Ø±ÛŒÙ\n",
            "========================\n",
            "Ù…Ø§ ØµØ§Ø­Ø¨Ø®Ø§Ù†Ù‡ Ù‡Ø³ØªÛŒÙ…\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#MahsaAmini \n",
            "#Mahsa_Amini \n",
            "#OpIran\n",
            "========================\n",
            "@negarkardan @oraclenik Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†\n",
            "\n",
            "#Mahsa_Amini\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ\n",
            "#OpIran\n",
            "========================\n",
            "#Ø¬Ø¯Ù‰ Ø§Ù…Ø±ÙˆØ² Ø¬Ùˆ Ø¨Ø§ÙŠØ¯Ù† Ø®Ø±ÙØª Ù†ØªÙˆÙ†Ø³Øª ØªØ§ Ù£ Ø¨Ø´Ù…Ø§Ø±Ù‡ ! Ø¨Ø¹Ø¯ Ù…ÙŠØ®ÙˆØ§Ø¯ Ù…Ø±Ø¯Ù… Ø´Ø¬Ø§Ø¹ Ø§ÙŠØ±Ø§Ù† Ø±Ùˆ Ù‡Ù…Ø±Ø§Ù‡Ù‰ ÙƒÙ†Ù‡ ! ØªÙˆ Ø¨Ù‡ Ø±Ø¦ÙŠØ³Ù‰ Ùˆ ÙƒØ«Ø§ÙØªÙ‡Ø§Ù‰ Ø¬Ù…Ù‡ÙˆØ±Ù‰ Ø§Ø³Ù„Ø§Ù…Ù‰ ÙˆÙŠØ²Ø§ Ù†Ø¯Ù‡ ØŒ ÙƒÙ…Ùƒ Ù¾ÙŠØ´ÙƒØ´Øª \n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ #Mahsa_Amini \n",
            "#Ù†ÙŠÙƒØ§_Ø´Ø§ÙƒØ±Ù…Ù‰ #Nika_Shakarami\n",
            "========================\n",
            "#Mahsa_Amini Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ù‡Ù… ÙˆØ·Ù† Ù‡Ø§ÛŒ Ø¯Ø± Ø¨Ù†Ø¯\n",
            "========================\n",
            "Ø³ÙˆÙ¾Ø± Ø§Ø³ØªØ§Ø± Ø¨ÛŒâ€ŒØ´Ø±ÙÛŒ Ø§ÛŒØ±Ø§Ù† \n",
            "\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#Mahsa_Amini \n",
            "#MahsaAmini https://t.co/H6paux9UsD\n"
          ]
        }
      ],
      "source": [
        "# 3. find 10 nearest tweets from chosen tweet.\n",
        "\n",
        "# Get the index of the chosen tweet in the tfidf_matrix\n",
        "chosen_tweet_index = np.where(Tweets == chosen_tweet)[0][0]\n",
        "\n",
        "# Get the TF-IDF vector for the chosen tweet\n",
        "chosen_tweet_vector = tf_idf_matrix[chosen_tweet_index]\n",
        "\n",
        "# Calculate the cosine similarity between the chosen tweet and all other tweets\n",
        "similarities = []\n",
        "for i in range(len(Tweets)):\n",
        "    similarity = cosine_similarity(chosen_tweet_vector.toarray()[0], tf_idf_matrix[i].toarray()[0])\n",
        "    similarities.append(similarity)\n",
        "\n",
        "# Sort the similarities and get the indices of the 10 nearest tweets\n",
        "nearest_indices = np.argsort(similarities)[::-1][1:11]\n",
        "\n",
        "print(\"10 Nearest Tweets:\")\n",
        "for index in nearest_indices:\n",
        "    print(\"========================\")\n",
        "    print(Tweets[index])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chosen_tweet = random.choice(Tweets)\n",
        "print(\"Chosen Tweet:\", chosen_tweet)\n",
        "\n",
        "# Get the index of the chosen tweet in the tfidf_matrix\n",
        "chosen_tweet_index = np.where(Tweets == chosen_tweet)[0][0]\n",
        "\n",
        "# Get the TF-IDF vector for the chosen tweet\n",
        "chosen_tweet_vector = tf_idf_matrix[chosen_tweet_index]\n",
        "\n",
        "# Calculate the cosine similarity between the chosen tweet and all other tweets\n",
        "similarities = []\n",
        "for i in range(len(Tweets)):\n",
        "    similarity = cosine_similarity(chosen_tweet_vector.toarray()[0], tf_idf_matrix[i].toarray()[0])\n",
        "    similarities.append(similarity)\n",
        "\n",
        "# Sort the similarities and get the indices of the 10 nearest tweets\n",
        "nearest_indices = np.argsort(similarities)[::-1][1:11]\n",
        "\n",
        "print(\"10 Nearest Tweets:\")\n",
        "for index in nearest_indices:\n",
        "    print(\"========================\")\n",
        "    print(Tweets[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq1wZeHdFjU3",
        "outputId": "4c48606f-5f9f-47e2-aafe-daf1316aba97"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen Tweet: Ø§Ù†Ù‚Ù„Ø§Ø¨ Ø²Ù†Ø§Ù† Ø´Ø¬Ø§Ø¹ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¨Ø§ Ø³Ø§Ù¾ÙˆØ±Øª Ù…Ø±Ø¯Ø§Ù† Ø´Ø¬Ø§Ø¹ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ† Ø¨Ø§Ø± Ø¯Ø± ØªØ§Ø±ÛŒØ® Ø¯Ù†ÛŒØ§ . \n",
            "Ù…ÙˆØ§Ø¸Ø¨ Ù‡Ù…Ø¯ÛŒÚ¯Ù‡ Ø¨Ø§Ø´ÛŒØ¯ .\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#Ù†Ù‡_Ø¨Ù‡_Ø¬Ù…Ù‡ÙˆØ±ÛŒ_Ø§Ø³Ù„Ø§Ù…ÙŠ \n",
            "#Mahsa_Aminiâ€Œ \n",
            "#MahsaAmini \n",
            "#NoToIslamicRepublic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-fb2fe9b1de3f>:16: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  cosine_similarity = dot_product / (norm_u * norm_v)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 Nearest Tweets:\n",
            "========================\n",
            "Ø¨Ø±Ø§ÛŒ Ø¨ØºØ¶ Ù…Ø§Ø¯Ø±Ø§Ù† Ú†Ø´Ù… Ø¨Ù‡ Ø±Ø§Ù‡ Ø¢Ø¨Ø§Ù†  #OpIran   #Mahsa_Amini\n",
            "========================\n",
            "Ø¨Ø±Ø§ÛŒ Ú©ÙˆØ¯Ú©Ø§Ù† Ú©Ø§Ø± Ø³Ø±Ø²Ù…ÛŒÙ†Ù…...\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ #OpIran  #Mahsa_Amini\n",
            "========================\n",
            "@javoone_piir Ù‚Ø±Ø¨ÙˆÙ† ØªÚ© ØªÚ©Ø´ÙˆÙ† Ø¨Ø±Ù…...\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#Mahsa_Amini \n",
            "#oplran\n",
            "========================\n",
            "@Navidrafieii #Mahsa_Amini #Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ #OpIran  Ø¯Ø±ÙˆØ¯ Ø¨ ØºÙŠØ±ØªØªÙˆÙ† ğŸ¥º\n",
            "========================\n",
            "@nik_yousefi Ø­Ø¯Ø§Ù‚Ù„Ø´ Ø§ÛŒÙ†Ù‡ Ú©Ù‡ Ù‡Ù…Ù‡ ÛŒ Ø®ÙˆØ¯Ù…ÙˆÙ† Ø±Ùˆ Ú¯Ø°Ø§Ø´ØªÛŒÙ… #Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ  #Mahsa_Amini\n",
            "========================\n",
            "@Frozen_njvn @Mohamad_esf20 #Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#Mahsa_Amini \n",
            "Ù…Ø§ Ø¨Ù‡ ÛŒØ§Ø¯ ØªÙˆ Ø®ÙˆØ§Ù‡ÛŒÙ… Ù…Ø§Ù†Ø¯.\n",
            "========================\n",
            "Ø²Ù…Ø§Ù† Ù…Ø¯Ø±Ø³Ù‡ Ù†Ø§Ø¸Ù… Ù†Ú¯Ø§Ù…ÙˆÙ† Ù…ÛŒÚ©Ø±Ø¯ Ø®ÙˆØ¯Ù…ÙˆÙ†Ùˆ Ø¬Ù…Ø¹ Ù…ÛŒÚ©Ø±Ø¯ÛŒÙ…\n",
            "Ø¨Ø§Ø¨Ø§ Ø¯Ø³Øª Ù…Ø±ÛŒØ²Ø§Ø¯ Ú©Ø§Ø´ ÛŒÙ‡ Ú¯ÙˆØ´Ù‡ Ø§Ø² Ø¬Ù†Ù…ØªÙˆÙ†Ùˆ Ø¯Ø§Ø´ØªÙ…\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#Mahsa_Amini \n",
            "#MahsaAmini\n",
            "========================\n",
            "Ø¨Ø±Ø§ÛŒ ÙÙ‚Ø±...\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#OpIran \n",
            "#Mahsa_Amini\n",
            "========================\n",
            "@bunchofwirdos Ø²Ù† Ø²Ù†Ø¯Ú¯ÛŒ Ø¢Ø²Ø§Ø¯ÛŒ \n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ ØŒ #Mahsa_Amini\n",
            "========================\n",
            "@ferani666 @e_mitraa ØªØ§ Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ù†Øª Ù‡Ø³Øª \n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#Mahsa_Amini\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqNCL4kFmvm3"
      },
      "source": [
        "##### Describe advantages and disadvantages of TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GHvpTAZu7ZU"
      },
      "source": [
        "**Advantages:**\n",
        "1. **Term Importance:** TF-IDF highlights important terms in a document by assigning higher weights to words that are more frequent in the document and less frequent in the entire corpus. This allows for effective keyword extraction and helps in identifying the most relevant terms within a document.\n",
        "\n",
        "2. **Document Similarity:** TF-IDF enables the calculation of cosine similarity between documents based on their TF-IDF vector representations. This similarity measure is useful for tasks such as document clustering, information retrieval, and recommendation systems.\n",
        "\n",
        "3. **Language Independence:** TF-IDF is language-independent, meaning it can be applied to documents in any language. It doesn't rely on language-specific rules or heuristics, making it a versatile technique for text analysis across different languages.\n",
        "\n",
        "4. **Computational Efficiency:** TF-IDF can be computed efficiently, especially when using sparse matrix representations. This makes it scalable for large corpora and enables fast retrieval of relevant documents based on query terms.\n",
        "\n",
        "**Disadvantages:**\n",
        "1. **Term Frequency Bias:** TF-IDF heavily relies on term frequency. Overly frequent terms within a document may dominate the TF-IDF score, potentially overshadowing other important terms. This can be mitigated by using term frequency normalization techniques.\n",
        "2. **Lack of Semantic Understanding:** TF-IDF does not capture the semantic meaning of words or the relationships between them. It treats each term independently, which may limit its ability to capture the context or nuanced meaning of phrases or multi-word expressions.\n",
        "3. **Handling Out-of-Vocabulary Words:** TF-IDF is based on a fixed vocabulary derived from the corpus. Out-of-vocabulary words, i.e., words not present in the vocabulary, are typically ignored or treated as noise. This can be a limitation when dealing with specialized or domain-specific terms.\n",
        "4. **Document Length Bias:** Longer documents tend to have higher term frequencies, which can bias the TF-IDF scores. Longer documents may have higher TF-IDF values simply due to more occurrences of terms, even if the terms are not necessarily more important.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INM6vtm2zqJs"
      },
      "source": [
        "# 4. Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TCnxqaVY2zCc"
      },
      "outputs": [],
      "source": [
        "# 1. train a word2vec model base on all tweets\n",
        "# Create a list of tokenized tweets\n",
        "tokenized_tweets = [tweet.split() for tweet in PureText_data]\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_tweets, vector_size=100, window=5, min_count=5, workers=4)\n",
        "\n",
        "# Save the trained model for future use\n",
        "model.save(\"tweet_word2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. find 10 nearest words from \"Ø¢Ø²Ø§Ø¯ÛŒ\"\n",
        "\n",
        "# Load the trained Word2Vec model\n",
        "model = Word2Vec.load(\"tweet_word2vec.model\")\n",
        "\n",
        "# Find the 10 nearest words to \"Ø¢Ø²Ø§Ø¯ÛŒ\"\n",
        "nearest_words = model.wv.most_similar(\"Ø¢Ø²Ø§Ø¯ÛŒ\", topn=10)\n",
        "\n",
        "# Print the nearest words\n",
        "print(\"10 Nearest Words to 'Ø¢Ø²Ø§Ø¯ÛŒ':\")\n",
        "for word, similarity in nearest_words:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQyDwe0zI7dM",
        "outputId": "9ae64b32-4647-4d58-f6d2-739dbfd312b1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 Nearest Words to 'Ø¢Ø²Ø§Ø¯ÛŒ':\n",
            "Ø²Ù†Ø¯Ú¯ÛŒØŒ\n",
            "Ø§Ø²Ø§Ø¯ÛŒ\n",
            "Ø²Ù†ØŒ\n",
            "Ø§ÛŒØ³ØªØ§Ø¯Ù†\n",
            "Ø®ÙˆØ§Ù‡\n",
            "Ø§ÛŒØ±Ø§Ù†\n",
            "ØŒØ²Ù†Ø¯Ú¯ÛŒ\n",
            "Ø²Ù†\n",
            "Ø¢Ø²Ø§Ø¯ÛŒØŒ\n",
            "Ø²Ù†Ø¯Ú¯ÛŒ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained Word2Vec model\n",
        "model = Word2Vec.load(\"tweet_word2vec.model\")\n",
        "\n",
        "# Find the 10 nearest words to \"Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±\"\n",
        "nearest_words = model.wv.most_similar(\"Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±\", topn=10)\n",
        "\n",
        "# Print the nearest words\n",
        "print(\"10 Nearest Words to 'Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±':\")\n",
        "for word, similarity in nearest_words:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nVGvag7JrhB",
        "outputId": "a8cec431-5a50-4a1e-887f-5c9b70623cb1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 Nearest Words to 'Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±':\n",
            "Ø§ÙˆÙ…Ø¯Ù†\n",
            "ÙˆØ§Ø³Ù‡\n",
            "ÙˆØ³Ø·\n",
            "Ø§Ø³Ù„Ø­Ù‡\n",
            "ÙØ±Ø§Ø±\n",
            "Ø¯Ø§Ø¯ÛŒÙ…\n",
            "Ø³Ù…Øª\n",
            "ÙÛŒÙ„ØªØ±\n",
            "Ù¾Ø§Ú©\n",
            "Ø²Ø¯ÛŒ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37oMUvZcmvm4"
      },
      "source": [
        "##### Describe advantages and disadvantages of Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv39LXf0wkjd"
      },
      "source": [
        "**Advantages:**\n",
        "1. **Capturing Semantic Relationships:** Word2Vec can capture semantic relationships between words by representing them as dense vectors in a continuous vector space. Similar words tend to have similar vector representations, enabling the model to capture word similarity and analogies.\n",
        "\n",
        "2. **Dimensionality Reduction:** Word2Vec reduces the dimensionality of word representations. Instead of representing words as one-hot vectors in a high-dimensional space, Word2Vec provides compact and dense vector representations that capture meaningful semantic information.\n",
        "\n",
        "3. **Contextual Information:** Word2Vec considers the context in which a word appears, allowing it to capture the meaning of words based on their surrounding words. This enables the model to capture syntactic and semantic relationships.\n",
        "\n",
        "4. **Efficiency:** Word2Vec uses an efficient implementation, such as the skip-gram or continuous bag-of-words (CBOW) models, which make it computationally efficient to train on large-scale datasets. Once trained, the model can quickly provide word embeddings for downstream tasks.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "1. **Lack of Subword Information:** Word2Vec treats words as atomic units and does not capture subword information. Rare or out-of-vocabulary words may not have meaningful embeddings, and the model may struggle with morphologically rich languages or words with multiple meanings.\n",
        "2. **Limited Context Window:** Word2Vec uses a fixed context window size to capture word relationships. This limits the model's ability to capture long-range dependencies or relationships between words that are further apart.\n",
        "3. **Domain-Specific Representations:** Word2Vec embeddings are trained on a specific corpus. If the target domain differs significantly from the training corpus, the embeddings may not capture the specific domain's nuances and may require additional fine-tuning or training on domain-specific data.\n",
        "4. **Polysemy and Homonymy:** Word2Vec treats each word as a single entity, ignoring potential multiple meanings or contexts. This can result in ambiguous representations for polysemous words or different senses of homonymous words.\n",
        "5. **Lack of Compositionality:** Word2Vec does not inherently capture compositional meaning, where the meaning of a phrase or sentence is derived from the combination of individual word meanings. It treats each word independently, limiting its ability to capture complex linguistic structures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSdlWMl64aPN"
      },
      "source": [
        "# 5. Contextualized embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zgVAEQhyOPxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d15210d-408d-4dd0-ca50-fea97178f33e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GfKEqNml6eEB"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "0t3Q_ewkQUbO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file with the sentiment data\n",
        "data = pd.read_csv('Q3_data.csv')\n",
        "texts = data['Text']\n",
        "labels = data['Sentiment']\n",
        "\n",
        "# Map string labels to integers\n",
        "label_map = {\n",
        "    'negative': 0,\n",
        "    'very negative': 1,\n",
        "    'positive': 2,\n",
        "    'no sentiment expressed': 3,\n",
        "    'very positive': 4,\n",
        "    'mixed': 5\n",
        "}\n",
        "\n",
        "labels = labels.map(label_map)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "qMPL6mibRfvs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define a custom dataset for sentiment classification\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'label': torch.tensor(label)\n",
        "        }"
      ],
      "metadata": {
        "id": "6gL8iRHcRrAr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create instances of the custom dataset for training and validation\n",
        "train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = CustomDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "# Define the BERT model for sentiment classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)\n",
        "\n",
        "# Define the optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7wY8cLMRv3W",
        "outputId": "0eb9e3b3-a7f5-4cf9-a4d2-c5839c5660d9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training function\n",
        "def train(model, train_dataset, val_dataset, optimizer, scheduler, num_epochs=3, batch_size=32):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "        train_loss /= len(train_dataset)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct_preds = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits\n",
        "\n",
        "                val_loss += loss.item() * input_ids.size(0)\n",
        "                _, predicted_labels = torch.max(logits, dim=1)\n",
        "                correct_preds += torch.sum(predicted_labels == labels).item()\n",
        "\n",
        "        val_loss /= len(val_dataset)\n",
        "        val_accuracy = correct_preds / len(val_dataset)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "        scheduler.step()"
      ],
      "metadata": {
        "id": "_BudvlIRR2Ki"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. fine-tune the model base on all tweets\n",
        "# Fine-tune the BERT model for sentiment classification\n",
        "train(model, train_dataset, val_dataset, optimizer, scheduler, num_epochs=3, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "TDJ0-kloQgXj",
        "outputId": "a95b6d51-3a04-4cee-bcef-18af9acf9b90"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "14602",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 14602",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-e28de1e4152d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1. fine-tune the model base on all tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Fine-tune the BERT model for sentiment classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-e04b0cb14f75>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, val_dataset, optimizer, scheduler, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-8f7f1f465f16>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 14602"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Read the CSV file with the sentiment data\n",
        "data = pd.read_csv('Q3_data.csv')\n",
        "\n",
        "# Reset the index of the DataFrame\n",
        "data = data.reset_index(drop=True)\n",
        "\n",
        "texts = data['Text']\n",
        "labels = data['Sentiment']\n",
        "\n",
        "# Map string labels to integers\n",
        "label_map = {\n",
        "    'negative': 0,\n",
        "    'very negative': 1,\n",
        "    'positive': 2,\n",
        "    'no sentiment expressed': 3,\n",
        "    'very positive': 4,\n",
        "    'mixed': 5\n",
        "}\n",
        "\n",
        "labels = labels.map(label_map)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define a custom dataset for sentiment classification\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=128,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'label': torch.tensor(label)\n",
        "        }\n",
        "\n",
        "# Create instances of the custom dataset for training and validation\n",
        "train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = CustomDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "# Define the BERT model for sentiment classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)\n",
        "\n",
        "# Define the optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "\n",
        "# Define the training function\n",
        "def train(model, train_dataset, val_dataset, optimizer, scheduler, num_epochs=3, batch_size=32):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "        train_loss /= len(train_dataset)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct_preds = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits\n",
        "\n",
        "                val_loss += loss.item() * input_ids.size(0)\n",
        "                _, predicted_labels = torch.max(logits, dim=1)\n",
        "                correct_preds += torch.sum(predicted_labels == labels).item()\n",
        "\n",
        "        val_loss /= len(val_dataset)\n",
        "        val_accuracy = correct_preds / len(val_dataset)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "train(model, train_dataset, val_dataset, optimizer, scheduler, num_epochs=3, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "njKJZGX7SkKR",
        "outputId": "690f59e0-d7f9-41b3-c6d8-ccff1c9c7e91"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "5198",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 5198",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-f51b393ca6b0>\u001b[0m in \u001b[0;36m<cell line: 133>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-f51b393ca6b0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, val_dataset, optimizer, scheduler, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-f51b393ca6b0>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 5198"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. find 10 nearest words from \"Ø¢Ø²Ø§Ø¯ÛŒ\""
      ],
      "metadata": {
        "id": "BGAb56yCMFR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define the input text\n",
        "input_text = \"Ø¢Ø²Ø§Ø¯ÛŒ\"\n",
        "\n",
        "# Tokenize the input text\n",
        "tokenized_text = tokenizer.tokenize(input_text)\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Convert tokens to tensor\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "# Generate the contextualized embeddings\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "\n",
        "# Get the embeddings for the input tokens\n",
        "input_embedding = outputs[0][0]  # Embedding for the first token, which represents the input word\n",
        "\n",
        "# Calculate cosine similarity between the input word embedding and each tweet\n",
        "similarities = []\n",
        "for tweet in PureText_data:\n",
        "    # Tokenize and convert tweet to tensor\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "    tweet_indexed_tokens = tokenizer.convert_tokens_to_ids(tweet_tokens)\n",
        "    tweet_tensor = torch.tensor([tweet_indexed_tokens])\n",
        "\n",
        "    # Generate the contextualized embeddings for the tweet\n",
        "    with torch.no_grad():\n",
        "        tweet_outputs = model(tweet_tensor)\n",
        "\n",
        "    # Get the embeddings for the tweet tokens\n",
        "    tweet_embeddings = tweet_outputs[0][0]  # Embeddings for the tweet tokens\n",
        "\n",
        "    # Calculate cosine similarity between input word embedding and tweet embeddings\n",
        "    similarity = np.dot(input_embedding.numpy(), tweet_embeddings.numpy().T) / (\n",
        "        np.linalg.norm(input_embedding.numpy()) * np.linalg.norm(tweet_embeddings.numpy(), axis=1)\n",
        "    )\n",
        "    similarities.append(similarity)\n",
        "\n",
        "# Sort the tweets based on cosine similarity in descending order\n",
        "sorted_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "# Get the 10 most similar tweets\n",
        "nearest_tweets = [PureText_data[index] for index in sorted_indices[:10]]\n",
        "\n",
        "# Print the nearest tweets\n",
        "for tweet in nearest_tweets:\n",
        "    print(tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "9d4mcxUsNILs",
        "outputId": "7d542a6e-5167-4b6d-cf2b-e6b7f880d055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (20000, 1) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-0b18ca666682>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Sort the tweets based on cosine similarity in descending order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0msorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Get the 10 most similar tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \"\"\"\n\u001b[0;32m-> 1133\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argsort'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (20000, 1) + inhomogeneous part."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBUkqdX0mvm6"
      },
      "source": [
        "##### Describe advantages and disadvantages of Contextualized embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al-4HT8KxTwd"
      },
      "source": [
        "Advantages:\n",
        "\n",
        "\n",
        "Disadvantages:\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}