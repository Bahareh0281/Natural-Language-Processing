{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8Wio74Smvmm"
      },
      "source": [
        "1.Submit a Google Colab notebook containing your completed code and experimentation results.\n",
        "\n",
        "2.Include comments and explanations in your code to help understand the implemented logic.\n",
        "\n",
        "**Additional Notes:**\n",
        "*   Ensure that the notebook runs successfully in Google Colab.\n",
        "*   Document any issues encountered during experimentation and how you addressed them.\n",
        "\n",
        "**Grading:**\n",
        "*   Each task will be graded out of the specified points.\n",
        "*   Points will be awarded for correctness, clarity of code, thorough experimentation, and insightful analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oCb3cVwzrfbb"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# SOURCE_DIR = '/content/Q3_data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HtCgCrUVtU_J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uEa_gZkKmvmv"
      },
      "outputs": [],
      "source": [
        "def delete_hashtag_usernames(text):\n",
        "  try:\n",
        "    result = []\n",
        "    for word in text.split():\n",
        "      if word[0] not in ['@', '#']:\n",
        "        result.append(word)\n",
        "    return ' '.join(result)\n",
        "  except:\n",
        "    return ''\n",
        "\n",
        "def delete_url(text):\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "  return text\n",
        "\n",
        "def delete_ex(text):\n",
        "  text = re.sub(r'\\u200c', '', text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz4cWWgA4xBF"
      },
      "source": [
        "# 0. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0FjQ9_ZMkve",
        "outputId": "5205c825-ea79-4011-bc91-2517eac468d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: json-lines in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from json-lines) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install json-lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Uf7K92-olSfe"
      },
      "outputs": [],
      "source": [
        "import json_lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bG5awXKo40HS"
      },
      "outputs": [],
      "source": [
        "# 1. extract all tweets from file and save them in memory\n",
        "# 2. remove urls, hashtags and usernames. use the prepared functions\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv('Q3_data.csv')\n",
        "# print(data.columns)\n",
        "\n",
        "PureText_data = data['PureText']\n",
        "\n",
        "# Apply preprocessing functions to the tweet data\n",
        "PureText_data = PureText_data.apply(delete_hashtag_usernames)\n",
        "PureText_data = PureText_data.apply(delete_url)\n",
        "PureText_data = PureText_data.apply(delete_ex)\n",
        "\n",
        "# Print the preprocessed tweet data\n",
        "# print(data['Text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cncc0Uxvmvmy"
      },
      "source": [
        "# 1. Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZjVk9qgmvmz"
      },
      "source": [
        "## Cosine Similarity\n",
        "\n",
        "To measure the similarity between two words, you need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows:\n",
        "\n",
        "$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta)Â \\tag{1}$$\n",
        "\n",
        "* $u \\cdot v$ is the dot product (or inner product) of two vectors\n",
        "* $||u||_2$ is the norm (or length) of the vector $u$\n",
        "* $\\theta$ is the angle between $u$ and $v$.\n",
        "* The cosine similarity depends on the angle between $u$ and $v$.\n",
        "    * If $u$ and $v$ are very similar, their cosine similarity will be close to 1.\n",
        "    * If they are dissimilar, the cosine similarity will take a smaller value.\n",
        "\n",
        "<img src=\"images/cosine_sim.png\" style=\"width:800px;height:250px;\">\n",
        "<caption><center><font color='purple'><b>Figure 1</b>: The cosine of the angle between two vectors is a measure of their similarity.</font></center></caption>\n",
        "\n",
        "Implement the function `cosine_similarity()` to evaluate the similarity between word vectors.\n",
        "\n",
        "**Reminder**: The norm of $u$ is defined as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dfiK-Lw7mvm0"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(u, v):\n",
        "    \"\"\"\n",
        "    Cosine similarity reflects the degree of similarity between u and v\n",
        "\n",
        "    Arguments:\n",
        "        u -- a word vector of shape (n,)\n",
        "        v -- a word vector of shape (n,)\n",
        "\n",
        "    Returns:\n",
        "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
        "    \"\"\"\n",
        "\n",
        "    dot_product = np.dot(u, v)\n",
        "    norm_u = np.sqrt(np.sum(u**2))\n",
        "    norm_v = np.sqrt(np.sum(v**2))\n",
        "    cosine_similarity = dot_product / (norm_u * norm_v)\n",
        "\n",
        "    return cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLDGkeBRmvm0"
      },
      "source": [
        "## find k nearest neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EaO0XQvKmvm1"
      },
      "outputs": [],
      "source": [
        "def find_k_nearest_neighbors(word, embedding_dict, k):\n",
        "  \"\"\"\n",
        "    implement a function to return the nearest words to an specific word based on the given dictionary\n",
        "\n",
        "    Arguments:\n",
        "        word           -- a word, string\n",
        "        embedding_dict -- dictionary that maps words to their corresponding vectors\n",
        "        k              -- the number of word that should be returned\n",
        "\n",
        "    Returns:\n",
        "        a list of size k consisting of the k most similar words to the given word\n",
        "\n",
        "    Note: use the cosine_similarity function that you have implemented to calculate the similarity between words\n",
        "    \"\"\"\n",
        "  # Ensure the word is in the embedding dictionary\n",
        "  if word not in embedding_dict:\n",
        "      return []\n",
        "\n",
        "  # Get the embedding for the word\n",
        "  word_embedding = embedding_dict[word]\n",
        "\n",
        "  # Calculate cosine similarity with all other words\n",
        "  similarities = {}\n",
        "  for other_word, other_embedding in embedding_dict.items():\n",
        "      # print(\"Other word is: \", other_word)\n",
        "      # print(\"other_embedding is: \", other_embedding)\n",
        "      if other_word != word:\n",
        "          sim = cosine_similarity(word_embedding, other_embedding)\n",
        "          similarities[other_word] = sim\n",
        "          # if sim != 0.0:\n",
        "            # print(\"sim is: \", sim)\n",
        "  # print (\"Similarities is: \", similarities)\n",
        "  # Sort by similarity\n",
        "  sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "  # Extract the top k words\n",
        "  # neighbors = [word for word, _ in sorted_similarities[:k]]\n",
        "  neighbors = sorted_similarities[:k]\n",
        "\n",
        "  return neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqr5DLDYuKd-"
      },
      "source": [
        "# 2. One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. find one hot encoding of each word\n",
        "\n",
        "words = PureText_data.str.split().tolist()\n",
        "words = [word for sublist in words for word in sublist]\n",
        "# print(words[1003])\n",
        "\n",
        "# Reshape the words to be a column vector\n",
        "words_array = np.array(words).reshape(-1, 1)\n",
        "# print(words_array)\n",
        "\n",
        "# Create the encoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Fit and transform the words to one-hot encoded vectors\n",
        "one_hot_encoded = encoder.fit_transform(words_array)\n",
        "\n",
        "# # Create a DataFrame to view the one-hot encoded words\n",
        "# one_hot_df = pd.DataFrame(one_hot_encoded, index=words, columns=encoder.get_feature_names_out())\n",
        "\n",
        "# # Display the one-hot encoded DataFrame\n",
        "# print(one_hot_df)"
      ],
      "metadata": {
        "id": "qIZzhK_El9Al",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46875915-95d6-4e7c-ec85-98987e1de29a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPqc0I0yuNlI",
        "outputId": "d12ac975-a008-4d03-c9e1-32bc6b3d4be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Ø¨Ù†Ø´ÛŒÙ†', 0.0), ('ØªØ§', 0.0), ('Ø´ÙˆØ¯', 0.0), ('Ù†Ù‚Ø´', 0.0), ('ÙØ§Ù„', 0.0), ('Ù…Ø§', 0.0), ('Ù‡Ù…', 0.0), ('ÙØ±Ø¯Ø§', 0.0), ('Ø´Ø¯Ù†', 0.0), ('Ø§ÛŒÙ†', 0.0)]\n",
            "[('Ø¨Ù†Ø´ÛŒÙ†', 0.0), ('ØªØ§', 0.0), ('Ø´ÙˆØ¯', 0.0), ('Ù†Ù‚Ø´', 0.0), ('ÙØ§Ù„', 0.0), ('Ù…Ø§', 0.0), ('Ù‡Ù…', 0.0), ('ÙØ±Ø¯Ø§', 0.0), ('Ø´Ø¯Ù†', 0.0), ('Ø§ÛŒÙ†', 0.0)]\n"
          ]
        }
      ],
      "source": [
        "# 2. find 10 nearest words from \"Ø¢Ø²Ø§Ø¯ÛŒ\"\n",
        "embedding_dict = {word: encoding for word, encoding in zip(words, one_hot_encoded)}\n",
        "\n",
        "word = \"Ø¢Ø²Ø§Ø¯ÛŒ\"\n",
        "k = 10\n",
        "nearest_words = find_k_nearest_neighbors(word, embedding_dict, k)\n",
        "print(nearest_words)\n",
        "\n",
        "word = \"Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±\"\n",
        "nearest_words = find_k_nearest_neighbors(word, embedding_dict, k)\n",
        "print(nearest_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each and every vector in the one hot encoding is orthogonal to each other. So the cosine similarity as well as distance between any two vectors are same. Thus it holds no relationship among them.\n",
        "That is why the nearest words found are the same. The cosine similarity of each pair of words equals 0."
      ],
      "metadata": {
        "id": "_yinx6QW8EfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just testing\n",
        "count = 0\n",
        "for value in embedding_dict['Ø¢Ø²Ø§Ø¯ÛŒ']:\n",
        "  if value != 0.0:\n",
        "    print(count, value)\n",
        "  count = count + 1\n",
        "\n",
        "count = 0\n",
        "for value in embedding_dict['Ø¨Ù‡Ø§Ø±Ù‡']:\n",
        "  if value != 0.0:\n",
        "    print(count, value)\n",
        "  count = count + 1\n",
        "\n",
        "count = 0\n",
        "for value in embedding_dict['Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±']:\n",
        "  if value != 0.0:\n",
        "    print(count, value)\n",
        "  count = count + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pasR5WChpOKV",
        "outputId": "ebf47634-0459-421d-c272-c1df208508fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1805 1.0\n",
            "7146 1.0\n",
            "28946 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FayPXc1mvm2"
      },
      "source": [
        "##### Describe advantages and disadvantages of one-hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9tAFn4brKrB"
      },
      "source": [
        "**Advantages:**\n",
        "\n",
        "1. **Simplicity:** One-hot encoding is a straightforward and simple method to represent categorical variables. It involves creating a binary vector where each element corresponds to a unique category, making it easy to understand and implement.\n",
        "\n",
        "2. **Retains categorical information:** One-hot encoding preserves the categorical nature of the variable. Each category is represented by a separate binary variable, allowing models to capture relationships and patterns specific to each category.\n",
        "\n",
        "3. **Compatibility with machine learning algorithms:** Many machine learning algorithms require numerical inputs. One-hot encoding converts categorical variables into a numeric format that can be readily used by these algorithms.\n",
        "\n",
        "4. **Avoids ordinality assumption:** One-hot encoding treats all categories as independent and does not impose any ordinal relationship between categories. This is useful when there is no inherent order or hierarchy among the categories.\n",
        "\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "1. **Dimensionality:** One-hot encoding expands the dimensionality of the feature space. If a categorical variable has a large number of unique categories, the resulting one-hot encoded representation can lead to a high-dimensional feature space, which may impact computational efficiency and model complexity.\n",
        "\n",
        "2. **Curse of dimensionality:** The increase in dimensionality due to one-hot encoding can lead to the curse of dimensionality. This refers to the problem where the number of features becomes large relative to the number of observations, which can result in sparse data, increased model complexity, and overfitting.\n",
        "\n",
        "3. **Redundancy:** One-hot encoding can introduce redundancy in the data representation. Since each category is represented by a separate binary variable, there is a perfect correlation between these variables. This redundancy can lead to multicollinearity issues in some models.\n",
        "\n",
        "4. **Handling new categories:** One-hot encoding requires defining the set of categories in advance. If new categories appear during testing or deployment, the one-hot encoding scheme may not handle them properly. This can be particularly problematic in real-world scenarios where new categories may emerge over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHeSYFUKw5gw"
      },
      "source": [
        "# 3. TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tweets = data['Text']\n",
        "print(Tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WUmrM9dsOMS",
        "outputId": "8d37cbba-6158-4efd-f942-b0f9ff20da34"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        Ø¨Ù†Ø´ÛŒÙ† ØªØ§ Ø´ÙˆØ¯ Ù†Ù‚Ø´ ÙØ§Ù„ Ù…Ø§ \\nÙ†Ù‚Ø´ Ù‡Ù…â€Œ ÙØ±Ø¯Ø§ Ø´Ø¯Ù†\\n#Ù…...\n",
            "1        @Tanasoli_Return @dr_moosavi Ø§ÛŒÙ† Ú¯ÙˆØ²Ùˆ Ø±Ùˆ Ú©ÛŒ Ú¯Ø±...\n",
            "2        @ghazaleghaffary Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ØŒ Ø¨Ø±Ø§ÛŒ Ù…Ù‡Ø³Ø§.\\n#OpIr...\n",
            "3        @_hidden_ocean Ù…Ø±Ú¯ Ø¨Ø± Ø¯ÛŒÚ©ØªØ§ØªÙˆØ± \\n#OpIran \\n#Ma...\n",
            "4        Ù†Ø°Ø§Ø±ÛŒÙ… Ø®ÙˆÙ†Ø´ÙˆÙ† Ù¾Ø§ÛŒÙ…Ø§Ù„ Ø´Ù‡.â€Œâ€Œ.â€Œâ€Œ.\\n#Mahsa_Amini #...\n",
            "                               ...                        \n",
            "19995    Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ±Ø§Ù† Ø¨Ø§Ù†Ùˆ #Mahsa_Amini      #MahsaAmini ...\n",
            "19996    @MohammadTehra16 @mimpedram Ø§Ø² Ø¨Ø³ Ø­Ø§Ø¬ Ø®Ø§Ù†Ù… Ø¯Ø±Ø§...\n",
            "19997    Ø¨Ù‡ Ø§ÙØªØ®Ø§Ø± Ø§Ø² Ø¨ÛŒÙ† Ø±ÙØªÙ† Ø¬Ù…Ù‡ÙˆØ±ÛŒ Ø§Ø³Ù„Ø§Ù…ÛŒğŸ™†â€â™‚ï¸ğŸ™†â€â™‚ï¸ğŸ™†â€â™‚...\n",
            "19998    Ù¾Ù†Ø¬Ø§Ù‡ Ùˆ Ø´ÛŒØ´ \\n\\n#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \\n#Mahsa_Amini \\n#...\n",
            "19999    Ø¯Ø± Ù…Ø­ÛŒØ· Ø·ÙˆÙØ§Ù†â€ŒØ²Ø§ÛŒ Ù…Ø§Ù‡Ø±Ø§Ù†Ù‡ Ø¯Ø± Ø¬Ù†Ú¯ Ø§Ø³Øª\\nÙ†Ø§Ø®Ø¯Ø§ÛŒ Ø§...\n",
            "Name: Text, Length: 20000, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. find the TF-IDF of all tweets.\n",
        "########## import and preprocess (PureText_data)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# # Create a document-term matrix\n",
        "# vectorizer = TfidfVectorizer()\n",
        "# document_term_matrix = vectorizer.fit_transform(Tweets)\n",
        "\n",
        "# # Calculate the TF-IDF values\n",
        "# tfidf_values = document_term_matrix.toarray()\n",
        "\n",
        "# # Normalize the TF-IDF values\n",
        "# normalized_tfidf = tfidf_values / np.linalg.norm(tfidf_values, axis=1, keepdims=True)\n",
        "\n",
        "# # Print the TF-IDF values for the first 10 tweets\n",
        "# for i in range(10):\n",
        "#     print(\"Tweet:\", Tweets[i])\n",
        "#     print(\"TF-IDF:\", normalized_tfidf[i])\n",
        "#     print()\n",
        "\n",
        "# Create a TfidfVectorizer object and fit it to the preprocessed corpus\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(words)\n",
        "\n",
        "# Transform the preprocessed corpus into a TF-IDF matrix\n",
        "tf_idf_matrix = vectorizer.transform(words)\n",
        "\n",
        "# Get list of feature names that correspond to the columns in the TF-IDF matrix\n",
        "print(\"Feature Names:\\n\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print the resulting matrix\n",
        "print(\"TF-IDF Matrix:\\n\", tf_idf_matrix.toarray())\n",
        "# for i in tf_idf_matrix.toarray():\n",
        "#   for j in i:\n",
        "#     if (j != 0):\n",
        "#       print (j)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yFr8FMAsbpJ",
        "outputId": "7a525a7f-aeec-4541-cca6-d56025d3d536"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names:\n",
            " ['00' '0020115687' '00971562643674' ... 'Û¹Û¸' 'Û¹Û¹' 'ïºØ³Øª']\n",
            "TF-IDF Matrix:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. choose one tweets randomly.\n",
        "import random\n",
        "chosen_tweet = random.choice(Tweets)\n",
        "print(\"Chosen Tweet:\", chosen_tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUJVtLSeE_yH",
        "outputId": "64ad3165-fcae-44c9-e73b-85738bd0f847"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen Tweet: @i6kitt1e Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "Ø¯Ø®ØªØ± Ø§ÛŒØ±Ø§Ù†ğŸ–¤ğŸ–¤\n",
            "#Mahsa_Amini\n",
            "#OpIran\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SJMju0Tiw9YA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eb6c0dd-abee-4ff4-bcac-0e6154d42f03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-fb2fe9b1de3f>:16: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  cosine_similarity = dot_product / (norm_u * norm_v)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 Nearest Tweets:\n",
            "========================\n",
            "Ø¨Ø±Ø§ÛŒ ÙˆÙ‚ØªØ§ÛŒÛŒ Ú©Ù‡ Ø®ÙˆØ§Ø³ØªÛŒÙ… Ù‡Ù…Ùˆ Ø¨ØºÙ„Ø¯Ú©Ù†ÛŒÙ… Ù…ÙˆÙ‚Ø¹ Ø®Ø¯Ø§ÙØ¸ÛŒ Ùˆ Ù†Ù…ÛŒØ´Ø¯\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#OpIran \n",
            "#MahsaAmini \n",
            "#Mahsa_Amini\n",
            "========================\n",
            "Ø¨Ø±Ø§ÛŒ Ø¢Ø²Ø§Ø¯ÛŒ\n",
            "Ø¨Ø±Ø§ÛŒ Ø­Ù‚ Ø§Ù†ØªØ®Ø§Ø¨ \n",
            "Ø¨Ø±Ø§ÛŒ ÛŒÙ‡ Ù†ÙØ³ Ø±Ø§Ø­Øª.... \n",
            "#MahsaAmini\n",
            "-#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ\n",
            "#Mahsa_Amini\n",
            "========================\n",
            "Ø´Ù…Ø§ Ù†Ù…ÙŠØªÙˆØ§Ù†ÙŠØ¯ Ø¨Ø§ Ø²Ø¨Ø§Ù† Ù…Ø¯Ù†ÙŠ Ùˆ Ù…Ù„Ø§ÙŠÙ… Ø§ÙŠØ±Ø§Ù† Ø±Ùˆ Ù†Ø¬Ø§Øª Ø¨Ø¯Ù‡ÙŠ Ø´Ù…Ø§ Ø¨Ø§ ÙŠÙƒ Ø­ÙŠÙˆØ§Ù† Ø·Ø±ÙÙŠ ÙƒÙ‡ ÙÙ‚Ø· Ø³Ù„Ø§Ø­ Ø¨Ø± Ø§Ùˆ ØºÙ„Ø¨Ù‡ Ù…ÙŠÙƒÙ†Ø¯ . #Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ  #Mahsa_Amini #Ø§Ø¹ØªØµØ§Ø¨_Ø³Ø±Ø§Ø³Ø±ÛŒ\n",
            "========================\n",
            "@hodalyyy @Ftmp191 Ø¨Ø±Ø§ÛŒ Ù…Ù‡Ø³Ø§.\n",
            "Ø¨Ø±Ø§ÛŒ Ø§Ø²Ø§Ø¯ÛŒ.\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ #Mahsa_Amini #OpIran\n",
            "========================\n",
            "Ø¯Ø³Ø®ÙˆÙˆÙˆÙˆÙˆÙˆÙˆÙˆØ´Ø´Ø´Ø´Ø´Ø´Ø´Ø´Ø´Ø´Ø´Ø´Ø´\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ #OpIran  #Mahsa_Amini\n",
            "========================\n",
            "RT besmaili: KÃ¶ln heute\n",
            "ØªØ¸Ø§Ù‡Ø±Ø§Øª Ø§Ù…Ø±ÙˆØ² Ø¯Ø± Ú©Ù„Ù† \n",
            "#IranRevolution #IranProtests2022 #Mahsa_Aminiâ€Œ #Ù…Ù‡Ø³Ø§_â€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€ŒØ§Ù…ÛŒÙ†ÛŒ https://t.câ€¦ \n",
            " #MahsaAmini #IranRevolution\n",
            "========================\n",
            "Ù…Ø±Ú¯ Ø¨Ø± Ø±ÛŒÛŒØ³ÛŒ\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ\n",
            "#Mahsa_Amini\n",
            "========================\n",
            "@this_ryouzaki ÙØ§Ù„Ùˆ Ú©Ù†ÛŒØ¯ Ø¨Ú© Ù…ÛŒØ¯Ù… \n",
            "\n",
            "#MahsaAmini \n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#Mahsa_Amini \n",
            "#OpIran\n",
            "========================\n",
            "@fans_raefipour Ù‡Ù… Ù…Ø§Ø¯Ø±ØªÙˆ Ú¯Ø§ÛŒÛŒØ¯Ù… ØªØ®Ù… Ø²Ù†Ø§ Ù¾Ø¯Ø± Ù†Ø§ Ù…Ø¹Ù„ÙˆÙ… #Mahsa_Amini\n",
            "========================\n",
            "@ActualFatemeh Ø¨Ø±Ø§ÛŒ Ø§Ø²Ø§Ø¯ÛŒ Ø§ÛŒØ±Ø§Ù†\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#Mahsa_Amini \n",
            "#OpIran\n"
          ]
        }
      ],
      "source": [
        "# 3. find 10 nearest tweets from chosen tweet.\n",
        "\n",
        "# Get the index of the chosen tweet in the tfidf_matrix\n",
        "chosen_tweet_index = np.where(Tweets == chosen_tweet)[0][0]\n",
        "\n",
        "# Get the TF-IDF vector for the chosen tweet\n",
        "chosen_tweet_vector = tf_idf_matrix[chosen_tweet_index]\n",
        "\n",
        "# Calculate the cosine similarity between the chosen tweet and all other tweets\n",
        "similarities = []\n",
        "for i in range(len(Tweets)):\n",
        "    similarity = cosine_similarity(chosen_tweet_vector.toarray()[0], tf_idf_matrix[i].toarray()[0])\n",
        "    similarities.append(similarity)\n",
        "\n",
        "# Sort the similarities and get the indices of the 10 nearest tweets\n",
        "nearest_indices = np.argsort(similarities)[::-1][1:11]\n",
        "\n",
        "print(\"10 Nearest Tweets:\")\n",
        "for index in nearest_indices:\n",
        "    print(\"========================\")\n",
        "    print(Tweets[index])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chosen_tweet = random.choice(Tweets)\n",
        "print(\"Chosen Tweet:\", chosen_tweet)\n",
        "\n",
        "# Get the index of the chosen tweet in the tfidf_matrix\n",
        "chosen_tweet_index = np.where(Tweets == chosen_tweet)[0][0]\n",
        "\n",
        "# Get the TF-IDF vector for the chosen tweet\n",
        "chosen_tweet_vector = tf_idf_matrix[chosen_tweet_index]\n",
        "\n",
        "# Calculate the cosine similarity between the chosen tweet and all other tweets\n",
        "similarities = []\n",
        "for i in range(len(Tweets)):\n",
        "    similarity = cosine_similarity(chosen_tweet_vector.toarray()[0], tf_idf_matrix[i].toarray()[0])\n",
        "    similarities.append(similarity)\n",
        "\n",
        "# Sort the similarities and get the indices of the 10 nearest tweets\n",
        "nearest_indices = np.argsort(similarities)[::-1][1:11]\n",
        "\n",
        "print(\"10 Nearest Tweets:\")\n",
        "for index in nearest_indices:\n",
        "    print(\"========================\")\n",
        "    print(Tweets[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq1wZeHdFjU3",
        "outputId": "3b25baa9-ee28-4e76-c4f1-6ec4e2d9e88d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen Tweet: @_3adr Ù‡Ø± Ø¬ÙˆØ±ÛŒ Ø´Ø¯Ù‡ Ø¨Ø²Ø§Ø± \n",
            "Ø±ÙˆØ­ÛŒÙ‡ Ù…ÛŒØ¯Ù‡ Ø¨ Ù…Ø±Ø¯Ù… \n",
            "#Mahsa_Amini \n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-fb2fe9b1de3f>:16: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  cosine_similarity = dot_product / (norm_u * norm_v)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 Nearest Tweets:\n",
            "========================\n",
            "Ø§Ù…Ø±ÙˆØ² Ø§Ø² ÛŒÙ‡ Ø¯Ø®ØªØ±Ù‡ 15 Ø³Ø§Ù„Ù‡ #ØªØ¨Ø±ÛŒØ² ÛŒ ÙˆØ³Ø· ØªØ¸Ø§Ù‡Ø±Ø§Øª Ù¾Ø±Ø³ÛŒØ¯Ù…ØŒ ÙˆØ§Ù‚Ø¹Ø§ Ù†Ù…ÛŒ ØªØ±Ø³ÛŒ ØŸØŸØŸ\n",
            "Ø¨Ø±Ú¯Ø´Øª Ú¯ÙØª: Ø§ÙˆÙ„Ù…Ø§Ø® ÙˆØ§Ø± Ø¯ÙˆÙ†Ù…Ø§Ø® ÛŒÙˆØ® (Ù…Ø±Ú¯ Ø±Ùˆ Ù‡Ø³ØªÙ… ÙˆÙ„ÛŒ Ø¨Ø±Ú¯Ø´Øª Ø±Ùˆ Ù†Ù‡ ...)\n",
            "\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#OpIran \n",
            "#Mahsa_Amini\n",
            "========================\n",
            "Ø¨Ù‡ Ø®Ø§Ø·Ø± Ù‡Ù…Ù‡ Ø´Ù‡ÛŒØ¯Ø§ÛŒÛŒ Ú©Ù‡ Ø§ÛŒÙ† Ú†Ù†Ø¯ Ø±ÙˆØ² Ùˆ ØªÙ…Ø§Ù… Ø§ÛŒÙ† Ø³Ø§Ù„Ù‡Ø§ Ø¯Ø§Ø¯ÛŒÙ…...\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#Mahsa_Amini \n",
            "#OpIran\n",
            "========================\n",
            "@Chandlershelby1 Ø¨Ø±Ø§ÛŒ\n",
            " #Mahsa_Amini \n",
            "#OpIran \n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ\n",
            "========================\n",
            "Ø§ÙˆÙ†Ø§ÛŒÛŒ Ú©Ù‡ Ø¯ÙˆØ³Øª Ù†Ø¯Ø§Ø±Ù… Ù¾ÛŒØ¬ Ø§ÛŒÙ†Ø³ØªØ§Ø´ÙˆÙ† Ø§Ø² Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø®Ø§Ø±Ø¬ Ø¨Ø´Ù‡ Ø¨Ø¬Ø§ÛŒ Ù†Ø§Ù„Ù‡ Ø¯Ø± Ø§ÛŒÙ† Ø²Ù…ÛŒÙ†Ù‡ØŒ ÙÙ‚Ø· Ú©Ø§ÙÛŒÙ‡ Ù¾Ø³Øª Ùˆ Ø§Ø³ØªÙˆØ±ÛŒ Ø­Ù…Ø§ÛŒØª Ø§Ø² Ù…Ø±Ø¯Ù… Ø¨Ø²Ø§Ø±Ù†ØŒ Ù‡Ù…ÛŒÙ†.\n",
            "#MahsaAmini\n",
            "#Mahsa_Amini\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ\n",
            "#OpIran\n",
            "========================\n",
            "Ø§ÛŒÙ†Ø§ÛŒÛŒ Ú©Ù‡ Ù…ÛŒÚ¯Ù† ÙÙ„Ø§Ù† Ø§Ø³ØªØ§Ù† Ùˆ ÙÙ„Ø§Ù† Ø¬Ø§ Ùˆ Ù…Ø±Ú©Ø² Ú©Ø´ÙˆØ± Ø®Ø¨Ø±ÛŒ Ù†ÛŒØ³Øª Ø§ÙˆÙ„ Ø¨Ø±Ù† Ø¨Ø¨ÛŒÙ†Ù† Ú†Ù‚Ø¯Ø± Ø¨Ú†Ù‡Ø§Ù…ÙˆÙ†Ùˆ Ø²Ø¯Ù† Ú©Ø´ØªÙ† Ø¨Ø¹Ø¯ Ø¨ÛŒØ§Ù† Ø§ÛŒÙ†Ùˆ Ø¨Ø®ÙˆØ±Ù†\n",
            "\n",
            "ØªÙØ±Ù‚Ù‡ Ø§ÙÚ©Ù† Ù‡Ø§ÛŒ Ø±Ùˆ Ø§Ø¹ØµØ§Ø¨ Ø³Ø§ÛŒØ¨Ø±ÛŒ\n",
            "\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#Mahsa_Amini\n",
            "========================\n",
            "@mamadporii Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡Ø§ Ø¨ÛŒØ¯Ø§Ø±Ù†\n",
            "Ø§ÛŒÙ†Ùˆ Û²Ûµ Ø±ÙˆØ²Ù‡ Ú©Ù‡ Ø¯Ø§Ø±ÛŒÙ… Ù…ÛŒØ¨ÛŒÙ†ÛŒÙ… Ùˆ Ù„Ù…Ø³ Ù…ÛŒÚ©Ù†ÛŒÙ…\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#Mahsa_Amini \n",
            "#OpIran\n",
            "========================\n",
            "Ù…Ø§ Ù‡Ù…Ù‡ Ù…Ù‡Ø³Ø§ Ù‡Ø³ØªÛŒÙ… Ø¨Ø¬Ù†Ú¯ ØªØ§ Ø¨Ø¬Ù†Ú¯ÛŒÙ…\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ\n",
            "#Mahsa_Amini \n",
            "#MahsaAmini\n",
            "========================\n",
            "@asemanhn @Cherii98 ÛŒÚ©\n",
            "Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "Ø¯Ø®ØªØ± Ø§ÛŒØ±Ø§Ù†ğŸ–¤ğŸ–¤\n",
            "#Mahsa_Amini\n",
            "#OpIran\n",
            "========================\n",
            "@OutFarsi @armin_prm #OpIran \n",
            "#Mahsa_Amini \n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "Ù…Ø§ Ù‡Ù…Ù‡ Ø¨Ø§Ù‡Ù… Ù‡Ø³ØªÛŒÙ…\n",
            "========================\n",
            "@Nafise1375 @Godofpersiian Ø®ÛŒÙ„ÛŒØ§ Ø¯Ø§Ø±Ù† Ù‡Ø´ØªÚ¯ Ø§Ø´ØªØ¨Ø§Ù‡ Ù…ÛŒØ²Ù†Ù†ØŒ Ø³Ø§ÛŒØ¨Ø±ÛŒ Ù‡Ù… Ù†ÛŒØ³ØªÙ†Â» Ù„Ø·ÙÙ† Ø¢Ú¯Ø§Ù‡Ø´ÙˆÙ† Ú©Ù†ÛŒÙ†...\n",
            "Ø¯Ù‚Øª Ú©Ù†ÛŒØ¯ #Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ Ø¯Ø±Ø³ØªÙ‡ ÛŒÙ‡ Ø¢Ù†Ø¯Ø±Ù„Ø§ÛŒÙ† Ø¨ÛŒØ´ØªØ± Ù†Ø¯Ø§Ø±Ù‡.#Ø§Ø¹ØªØµØ§Ø¨Ø§Øª_Ø³Ø±Ø§Ø±ÛŒ \n",
            "Ù‡Ø´ØªÚ¯ Ø±Ùˆ Ø®ÙˆØ¯ØªÙˆÙ† Ø¨Ù†ÙˆÛŒØ³ÛŒÙ†ØŒ Ø§Ø² Ø§Ù†ØªØ®Ø§Ø¨Ù‡Ø§ÛŒÛŒ Ú©Ù‡ ØªÙˆÛŒÛŒØªØ± Ø¨Ù‡ØªÙˆÙ† Ù…ÛŒØ¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒÙ†...\n",
            " #MashaAmini \n",
            "#OpIran\n",
            "#Mahsa_Amini\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqNCL4kFmvm3"
      },
      "source": [
        "##### Describe advantages and disadvantages of TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GHvpTAZu7ZU"
      },
      "source": [
        "**Advantages:**\n",
        "1. **Term Importance:** TF-IDF highlights important terms in a document by assigning higher weights to words that are more frequent in the document and less frequent in the entire corpus. This allows for effective keyword extraction and helps in identifying the most relevant terms within a document.\n",
        "\n",
        "2. **Document Similarity:** TF-IDF enables the calculation of cosine similarity between documents based on their TF-IDF vector representations. This similarity measure is useful for tasks such as document clustering, information retrieval, and recommendation systems.\n",
        "\n",
        "3. **Language Independence:** TF-IDF is language-independent, meaning it can be applied to documents in any language. It doesn't rely on language-specific rules or heuristics, making it a versatile technique for text analysis across different languages.\n",
        "\n",
        "4. **Computational Efficiency:** TF-IDF can be computed efficiently, especially when using sparse matrix representations. This makes it scalable for large corpora and enables fast retrieval of relevant documents based on query terms.\n",
        "\n",
        "**Disadvantages:**\n",
        "1. **Term Frequency Bias:** TF-IDF heavily relies on term frequency. Overly frequent terms within a document may dominate the TF-IDF score, potentially overshadowing other important terms. This can be mitigated by using term frequency normalization techniques.\n",
        "2. **Lack of Semantic Understanding:** TF-IDF does not capture the semantic meaning of words or the relationships between them. It treats each term independently, which may limit its ability to capture the context or nuanced meaning of phrases or multi-word expressions.\n",
        "3. **Handling Out-of-Vocabulary Words:** TF-IDF is based on a fixed vocabulary derived from the corpus. Out-of-vocabulary words, i.e., words not present in the vocabulary, are typically ignored or treated as noise. This can be a limitation when dealing with specialized or domain-specific terms.\n",
        "4. **Document Length Bias:** Longer documents tend to have higher term frequencies, which can bias the TF-IDF scores. Longer documents may have higher TF-IDF values simply due to more occurrences of terms, even if the terms are not necessarily more important.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INM6vtm2zqJs"
      },
      "source": [
        "# 4. Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TCnxqaVY2zCc"
      },
      "outputs": [],
      "source": [
        "# 1. train a word2vec model base on all tweets\n",
        "# Create a list of tokenized tweets\n",
        "tokenized_tweets = [tweet.split() for tweet in PureText_data]\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_tweets, vector_size=100, window=5, min_count=5, workers=4)\n",
        "\n",
        "# Save the trained model for future use\n",
        "model.save(\"tweet_word2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. find 10 nearest words from \"Ø¢Ø²Ø§Ø¯ÛŒ\"\n",
        "\n",
        "# Load the trained Word2Vec model\n",
        "model = Word2Vec.load(\"tweet_word2vec.model\")\n",
        "\n",
        "# Find the 10 nearest words to \"Ø¢Ø²Ø§Ø¯ÛŒ\"\n",
        "nearest_words = model.wv.most_similar(\"Ø¢Ø²Ø§Ø¯ÛŒ\", topn=10)\n",
        "\n",
        "# Print the nearest words\n",
        "print(\"10 Nearest Words to 'Ø¢Ø²Ø§Ø¯ÛŒ':\")\n",
        "for word, similarity in nearest_words:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQyDwe0zI7dM",
        "outputId": "5cbf30af-d124-43fc-b45e-2a65a12b6d5a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 Nearest Words to 'Ø¢Ø²Ø§Ø¯ÛŒ':\n",
            "Ø§Ø²Ø§Ø¯ÛŒ\n",
            "Ø²Ù†Ø¯Ú¯ÛŒØŒ\n",
            "Ø²Ù†ØŒ\n",
            "Ø²Ù†\n",
            "Ø®ÙˆØ§Ù‡\n",
            "Ø§ÛŒØ±Ø§Ù†\n",
            "Ø§Ø¨Ø§Ø¯ÛŒ\n",
            "Ø²Ù†Ø¯Ú¯ÛŒ\n",
            "Ø§ÛŒØ±Ø§Ù†Ù…\n",
            "Ø§Ù…ÛŒØ¯\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained Word2Vec model\n",
        "model = Word2Vec.load(\"tweet_word2vec.model\")\n",
        "\n",
        "# Find the 10 nearest words to \"Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±\"\n",
        "nearest_words = model.wv.most_similar(\"Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±\", topn=10)\n",
        "\n",
        "# Print the nearest words\n",
        "print(\"10 Nearest Words to 'Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±':\")\n",
        "for word, similarity in nearest_words:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nVGvag7JrhB",
        "outputId": "70e0d2cc-adc8-4107-9596-05a6c85cb5c4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 Nearest Words to 'Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±':\n",
            "Ø²Ù†Ú¯\n",
            "ÙˆÛŒØ¯ÛŒÙˆ\n",
            "Ø§ÛŒÙ†Ø¨Ø§Ø±\n",
            "Ú©Ø§Ø±Øª\n",
            "Ú©ÙˆØªØ§Ù‡\n",
            "Ù¾Ø´Øª\n",
            "Ù¾Ø§Ú©\n",
            "Ø±Ø§Ø­Øª\n",
            "Ù…ÛŒØªÙˆÙ†ÛŒÙ…\n",
            "Ù¾Ø³Ø±Ø§\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37oMUvZcmvm4"
      },
      "source": [
        "##### Describe advantages and disadvantages of Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv39LXf0wkjd"
      },
      "source": [
        "**Advantages:**\n",
        "1. **Capturing Semantic Relationships:** Word2Vec can capture semantic relationships between words by representing them as dense vectors in a continuous vector space. Similar words tend to have similar vector representations, enabling the model to capture word similarity and analogies.\n",
        "\n",
        "2. **Dimensionality Reduction:** Word2Vec reduces the dimensionality of word representations. Instead of representing words as one-hot vectors in a high-dimensional space, Word2Vec provides compact and dense vector representations that capture meaningful semantic information.\n",
        "\n",
        "3. **Contextual Information:** Word2Vec considers the context in which a word appears, allowing it to capture the meaning of words based on their surrounding words. This enables the model to capture syntactic and semantic relationships.\n",
        "\n",
        "4. **Efficiency:** Word2Vec uses an efficient implementation, such as the skip-gram or continuous bag-of-words (CBOW) models, which make it computationally efficient to train on large-scale datasets. Once trained, the model can quickly provide word embeddings for downstream tasks.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "1. **Lack of Subword Information:** Word2Vec treats words as atomic units and does not capture subword information. Rare or out-of-vocabulary words may not have meaningful embeddings, and the model may struggle with morphologically rich languages or words with multiple meanings.\n",
        "2. **Limited Context Window:** Word2Vec uses a fixed context window size to capture word relationships. This limits the model's ability to capture long-range dependencies or relationships between words that are further apart.\n",
        "3. **Domain-Specific Representations:** Word2Vec embeddings are trained on a specific corpus. If the target domain differs significantly from the training corpus, the embeddings may not capture the specific domain's nuances and may require additional fine-tuning or training on domain-specific data.\n",
        "4. **Polysemy and Homonymy:** Word2Vec treats each word as a single entity, ignoring potential multiple meanings or contexts. This can result in ambiguous representations for polysemous words or different senses of homonymous words.\n",
        "5. **Lack of Compositionality:** Word2Vec does not inherently capture compositional meaning, where the meaning of a phrase or sentence is derived from the combination of individual word meanings. It treats each word independently, limiting its ability to capture complex linguistic structures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSdlWMl64aPN"
      },
      "source": [
        "# 5. Contextualized embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zgVAEQhyOPxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c45a05-e294-4389-edc1-4144d71a5411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GfKEqNml6eEB"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "0t3Q_ewkQUbO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file with the sentiment data\n",
        "data = pd.read_csv('Q3_data.csv')\n",
        "texts = data['Text']\n",
        "labels = data['Sentiment']\n",
        "\n",
        "# Map string labels to integers\n",
        "label_map = {\n",
        "    'negative': 0,\n",
        "    'very negative': 1,\n",
        "    'positive': 2,\n",
        "    'no sentiment expressed': 3,\n",
        "    'very positive': 4,\n",
        "    'mixed': 5\n",
        "}\n",
        "\n",
        "labels = labels.map(label_map)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "qMPL6mibRfvs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define a custom dataset for sentiment classification\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.tweets = df['Text'].tolist()  # Convert the 'Text' column to a list\n",
        "        self.labels = df['Sentiment'].map({'negative': 0, 'very negative': 1, 'positive': 2, 'no sentiment expressed': 3, 'very positive': 4, 'mixed': 5}).tolist()  # Convert the 'Sentiment' column to a list\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tweet = self.tweets[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            tweet,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "6gL8iRHcRrAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1196ee0a-22a5-48aa-8317-e1e0a2854965"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create instances of the custom dataset for training and validation\n",
        "# train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
        "# val_dataset = CustomDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "# Define the BERT model for sentiment classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)\n",
        "\n",
        "# Define the optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7wY8cLMRv3W",
        "outputId": "01cebd21-20d6-4f5a-aff2-1e87d2cacc3a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset(data, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=16)"
      ],
      "metadata": {
        "id": "_BudvlIRR2Ki"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqO1QYCTiLR0",
        "outputId": "4e4e5041-f2cb-418d-af41-7d967dd880c7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "nYjQ_PCGiWDE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataloader, device):\n",
        "  \"\"\"\n",
        "  Function to perform validation on the model\n",
        "\n",
        "  Args:\n",
        "      model: The sentiment classification model\n",
        "      dataloader: The dataloader for the validation set\n",
        "      device: The device (CPU or GPU) to use\n",
        "\n",
        "  Returns:\n",
        "      The average validation loss\n",
        "  \"\"\"\n",
        "  model.eval()  # Set the model to evaluation mode\n",
        "  losses = []\n",
        "  with torch.no_grad():  # Disable gradient calculation for validation\n",
        "    for batch in tqdm(dataloader):\n",
        "      batch = {k: v.to(device) for k, v in batch.items()}\n",
        "      outputs = model(**batch)\n",
        "      loss_function = nn.CrossEntropyLoss()\n",
        "      loss = loss_function(outputs.logits, batch['labels'])\n",
        "      losses.append(loss.item())\n",
        "  return sum(losses) / len(losses)  # Calculate average validation loss"
      ],
      "metadata": {
        "id": "zPX7a70toOTk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "# optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Training loop with early stopping (optional)\n",
        "num_epochs = 10  # Set the number of training epochs\n",
        "patience = 3  # Number of epochs to wait for improvement before stopping (optional)\n",
        "\n",
        "best_loss = float('inf')\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#   print(f\"--- Epoch {epoch+1} ---\")\n",
        "\n",
        "#   # Print data lengths for debugging\n",
        "#   print(f\"Length of training texts: {len(train_texts)}\")\n",
        "#   print(f\"Length of training labels: {len(train_labels)}\")\n",
        "model.train()\n",
        "\n",
        "for batch in tqdm(dataloader):\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "    outputs = model(**batch)\n",
        "\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    loss = loss_function(outputs.logits, batch['labels'])\n",
        "\n",
        "    print(f\"Training Loss: {loss}\")  # Print training loss after each batch (optional)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # # Validation\n",
        "    # model.eval()  # Set model to evaluation mode again\n",
        "    # with torch.no_grad():\n",
        "    #   val_loss = validate(model, val_dataset, device)\n",
        "    # print(f\"Validation Loss: {val_loss}\")\n",
        "\n",
        "    # # Early stopping (optional)\n",
        "    # if val_loss < best_loss:\n",
        "    #   best_loss = val_loss\n",
        "    #   epochs_without_improvement = 0\n",
        "    # else:\n",
        "    #   epochs_without_improvement += 1\n",
        "    # if epochs_without_improvement >= patience:\n",
        "    #   print(\"Early stopping triggered\")\n",
        "    #   break\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ5BD5NYiYjD",
        "outputId": "514ad202-32b9-437b-909d-075ff41a00da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.739759922027588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/1250 [00:24<8:26:49, 24.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.9425362348556519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 2/1250 [00:46<8:01:10, 23.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.6889756917953491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 3/1250 [01:00<6:36:25, 19.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.661017656326294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 4/1250 [01:14<5:53:12, 17.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.7228882312774658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 5/1250 [01:29<5:34:44, 16.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.7215032577514648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 6/1250 [01:44<5:25:59, 15.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.631456732749939\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 7/1250 [01:58<5:15:04, 15.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.5778849124908447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 8/1250 [02:12<5:05:45, 14.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.7770164012908936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 9/1250 [02:26<4:59:13, 14.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.622807502746582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 10/1250 [02:39<4:55:11, 14.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.685881495475769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 11/1250 [02:54<4:55:58, 14.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.652949333190918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 12/1250 [03:08<4:52:41, 14.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.5635567903518677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 13/1250 [03:22<4:51:04, 14.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.491033911705017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 14/1250 [03:36<4:49:32, 14.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.4514977931976318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 15/1250 [03:50<4:49:55, 14.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.6338839530944824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|â–         | 16/1250 [04:04<4:48:40, 14.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.3752135038375854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|â–         | 17/1250 [04:17<4:46:52, 13.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.4200842380523682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|â–         | 18/1250 [04:31<4:45:46, 13.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.5077793598175049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|â–         | 19/1250 [04:45<4:47:16, 14.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.627557396888733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|â–         | 20/1250 [05:00<4:48:02, 14.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.5712246894836426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|â–         | 21/1250 [05:13<4:46:12, 13.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.2049825191497803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|â–         | 22/1250 [05:27<4:44:37, 13.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.5732909440994263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|â–         | 23/1250 [05:41<4:42:58, 13.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.2533743381500244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|â–         | 24/1250 [05:56<4:52:35, 14.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.5517573356628418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|â–         | 25/1250 [06:10<4:48:27, 14.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.7028117179870605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|â–         | 26/1250 [06:24<4:45:52, 14.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.5169092416763306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|â–         | 27/1250 [06:37<4:43:51, 13.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.4471746683120728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|â–         | 28/1250 [06:56<5:12:56, 15.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.3430225849151611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|â–         | 29/1250 [07:10<5:04:15, 14.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.363461971282959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|â–         | 30/1250 [07:24<4:58:29, 14.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.47640061378479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|â–         | 31/1250 [07:38<4:51:45, 14.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.4301021099090576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|â–         | 32/1250 [07:51<4:46:57, 14.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.5456136465072632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|â–         | 33/1250 [08:05<4:44:19, 14.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.3992462158203125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|â–         | 34/1250 [08:21<4:57:36, 14.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.4732033014297485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|â–         | 35/1250 [08:35<4:52:23, 14.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.4564619064331055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|â–         | 36/1250 [08:49<4:47:44, 14.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.3881242275238037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|â–         | 37/1250 [09:03<4:44:43, 14.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.7320538759231567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|â–         | 38/1250 [09:17<4:46:57, 14.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.520937442779541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|â–         | 39/1250 [09:31<4:44:14, 14.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.2901990413665771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|â–         | 40/1250 [09:45<4:42:34, 14.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.4177172183990479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|â–         | 41/1250 [09:59<4:41:36, 13.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.4929841756820679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|â–         | 42/1250 [10:13<4:41:51, 14.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.3231526613235474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|â–         | 43/1250 [10:26<4:39:52, 13.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.322385549545288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|â–         | 44/1250 [10:40<4:39:22, 13.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.322267770767212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|â–         | 45/1250 [10:54<4:38:10, 13.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.4564869403839111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|â–         | 46/1250 [11:08<4:38:24, 13.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.4160504341125488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|â–         | 47/1250 [11:22<4:39:07, 13.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.4505811929702759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|â–         | 48/1250 [11:36<4:39:36, 13.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.6229876279830933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|â–         | 49/1250 [11:50<4:38:21, 13.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.5026118755340576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|â–         | 50/1250 [12:04<4:36:59, 13.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.7393150329589844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|â–         | 51/1250 [12:18<4:37:41, 13.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.2649775743484497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|â–         | 52/1250 [12:31<4:36:22, 13.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.2141255140304565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|â–         | 53/1250 [12:45<4:35:41, 13.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.5424550771713257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|â–         | 54/1250 [12:59<4:35:38, 13.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.313936710357666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|â–         | 55/1250 [13:13<4:36:35, 13.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.3682994842529297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. fine-tune the model base on all tweets\n"
      ],
      "metadata": {
        "id": "TDJ0-kloQgXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. find 10 nearest words from \"Ø¢Ø²Ø§Ø¯ÛŒ\""
      ],
      "metadata": {
        "id": "BGAb56yCMFR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBUkqdX0mvm6"
      },
      "source": [
        "##### Describe advantages and disadvantages of Contextualized embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al-4HT8KxTwd"
      },
      "source": [
        "Advantages:\n",
        "\n",
        "\n",
        "Disadvantages:\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}