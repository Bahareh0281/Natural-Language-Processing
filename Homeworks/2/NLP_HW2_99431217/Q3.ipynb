{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8Wio74Smvmm"
      },
      "source": [
        "1.Submit a Google Colab notebook containing your completed code and experimentation results.\n",
        "\n",
        "2.Include comments and explanations in your code to help understand the implemented logic.\n",
        "\n",
        "**Additional Notes:**\n",
        "*   Ensure that the notebook runs successfully in Google Colab.\n",
        "*   Document any issues encountered during experimentation and how you addressed them.\n",
        "\n",
        "**Grading:**\n",
        "*   Each task will be graded out of the specified points.\n",
        "*   Points will be awarded for correctness, clarity of code, thorough experimentation, and insightful analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oCb3cVwzrfbb"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "# SOURCE_DIR = '/content/Q3_data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HtCgCrUVtU_J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uEa_gZkKmvmv"
      },
      "outputs": [],
      "source": [
        "def delete_hashtag_usernames(text):\n",
        "  try:\n",
        "    result = []\n",
        "    for word in text.split():\n",
        "      if word[0] not in ['@', '#']:\n",
        "        result.append(word)\n",
        "    return ' '.join(result)\n",
        "  except:\n",
        "    return ''\n",
        "\n",
        "def delete_url(text):\n",
        "  text = re.sub(r'http\\S+', '', text)\n",
        "  return text\n",
        "\n",
        "def delete_ex(text):\n",
        "  text = re.sub(r'\\u200c', '', text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz4cWWgA4xBF"
      },
      "source": [
        "# 0. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0FjQ9_ZMkve",
        "outputId": "c1b4a058-3ded-4302-bc64-bf995b656b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting json-lines\n",
            "  Downloading json_lines-0.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from json-lines) (1.16.0)\n",
            "Installing collected packages: json-lines\n",
            "Successfully installed json-lines-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install json-lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Uf7K92-olSfe"
      },
      "outputs": [],
      "source": [
        "import json_lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bG5awXKo40HS"
      },
      "outputs": [],
      "source": [
        "# 1. extract all tweets from file and save them in memory\n",
        "# 2. remove urls, hashtags and usernames. use the prepared functions\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv('Q3_data.csv')\n",
        "# print(data.columns)\n",
        "\n",
        "PureText_data = data['PureText']\n",
        "\n",
        "# Apply preprocessing functions to the tweet data\n",
        "PureText_data = PureText_data.apply(delete_hashtag_usernames)\n",
        "PureText_data = PureText_data.apply(delete_url)\n",
        "PureText_data = PureText_data.apply(delete_ex)\n",
        "\n",
        "# Print the preprocessed tweet data\n",
        "# print(data['Text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cncc0Uxvmvmy"
      },
      "source": [
        "# 1. Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZjVk9qgmvmz"
      },
      "source": [
        "## Cosine Similarity\n",
        "\n",
        "To measure the similarity between two words, you need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows:\n",
        "\n",
        "$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta)Â \\tag{1}$$\n",
        "\n",
        "* $u \\cdot v$ is the dot product (or inner product) of two vectors\n",
        "* $||u||_2$ is the norm (or length) of the vector $u$\n",
        "* $\\theta$ is the angle between $u$ and $v$.\n",
        "* The cosine similarity depends on the angle between $u$ and $v$.\n",
        "    * If $u$ and $v$ are very similar, their cosine similarity will be close to 1.\n",
        "    * If they are dissimilar, the cosine similarity will take a smaller value.\n",
        "\n",
        "<img src=\"images/cosine_sim.png\" style=\"width:800px;height:250px;\">\n",
        "<caption><center><font color='purple'><b>Figure 1</b>: The cosine of the angle between two vectors is a measure of their similarity.</font></center></caption>\n",
        "\n",
        "Implement the function `cosine_similarity()` to evaluate the similarity between word vectors.\n",
        "\n",
        "**Reminder**: The norm of $u$ is defined as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dfiK-Lw7mvm0"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(u, v):\n",
        "    \"\"\"\n",
        "    Cosine similarity reflects the degree of similarity between u and v\n",
        "\n",
        "    Arguments:\n",
        "        u -- a word vector of shape (n,)\n",
        "        v -- a word vector of shape (n,)\n",
        "\n",
        "    Returns:\n",
        "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
        "    \"\"\"\n",
        "\n",
        "    dot_product = np.dot(u, v)\n",
        "    norm_u = np.sqrt(np.sum(u**2))\n",
        "    norm_v = np.sqrt(np.sum(v**2))\n",
        "    cosine_similarity = dot_product / (norm_u * norm_v)\n",
        "\n",
        "    return cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLDGkeBRmvm0"
      },
      "source": [
        "## find k nearest neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EaO0XQvKmvm1"
      },
      "outputs": [],
      "source": [
        "def find_k_nearest_neighbors(word, embedding_dict, k):\n",
        "  \"\"\"\n",
        "    implement a function to return the nearest words to an specific word based on the given dictionary\n",
        "\n",
        "    Arguments:\n",
        "        word           -- a word, string\n",
        "        embedding_dict -- dictionary that maps words to their corresponding vectors\n",
        "        k              -- the number of word that should be returned\n",
        "\n",
        "    Returns:\n",
        "        a list of size k consisting of the k most similar words to the given word\n",
        "\n",
        "    Note: use the cosine_similarity function that you have implemented to calculate the similarity between words\n",
        "    \"\"\"\n",
        "  # Ensure the word is in the embedding dictionary\n",
        "  if word not in embedding_dict:\n",
        "      return []\n",
        "\n",
        "  # Get the embedding for the word\n",
        "  word_embedding = embedding_dict[word]\n",
        "\n",
        "  # Calculate cosine similarity with all other words\n",
        "  similarities = {}\n",
        "  for other_word, other_embedding in embedding_dict.items():\n",
        "      # print(\"Other word is: \", other_word)\n",
        "      # print(\"other_embedding is: \", other_embedding)\n",
        "      if other_word != word:\n",
        "          sim = cosine_similarity(word_embedding, other_embedding)\n",
        "          similarities[other_word] = sim\n",
        "          # if sim != 0.0:\n",
        "            # print(\"sim is: \", sim)\n",
        "  # print (\"Similarities is: \", similarities)\n",
        "  # Sort by similarity\n",
        "  sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "  # Extract the top k words\n",
        "  # neighbors = [word for word, _ in sorted_similarities[:k]]\n",
        "  neighbors = sorted_similarities[:k]\n",
        "\n",
        "  return neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqr5DLDYuKd-"
      },
      "source": [
        "# 2. One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. find one hot encoding of each word\n",
        "\n",
        "words = PureText_data.str.split().tolist()\n",
        "words = [word for sublist in words for word in sublist]\n",
        "# print(words[1003])\n",
        "\n",
        "# Reshape the words to be a column vector\n",
        "words_array = np.array(words).reshape(-1, 1)\n",
        "# print(words_array)\n",
        "\n",
        "# Create the encoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Fit and transform the words to one-hot encoded vectors\n",
        "one_hot_encoded = encoder.fit_transform(words_array)\n",
        "\n",
        "# # Create a DataFrame to view the one-hot encoded words\n",
        "# one_hot_df = pd.DataFrame(one_hot_encoded, index=words, columns=encoder.get_feature_names_out())\n",
        "\n",
        "# # Display the one-hot encoded DataFrame\n",
        "# print(one_hot_df)"
      ],
      "metadata": {
        "id": "qIZzhK_El9Al",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa6eb060-28b3-4fba-f327-602956b933ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPqc0I0yuNlI",
        "outputId": "0ce21674-0192-419c-8fbc-343ce04b709b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Ø¨Ù†Ø´ÛŒÙ†', 0.0), ('ØªØ§', 0.0), ('Ø´ÙˆØ¯', 0.0), ('Ù†Ù‚Ø´', 0.0), ('ÙØ§Ù„', 0.0), ('Ù…Ø§', 0.0), ('Ù‡Ù…', 0.0), ('ÙØ±Ø¯Ø§', 0.0), ('Ø´Ø¯Ù†', 0.0), ('Ø§ÛŒÙ†', 0.0)]\n",
            "[('Ø¨Ù†Ø´ÛŒÙ†', 0.0), ('ØªØ§', 0.0), ('Ø´ÙˆØ¯', 0.0), ('Ù†Ù‚Ø´', 0.0), ('ÙØ§Ù„', 0.0), ('Ù…Ø§', 0.0), ('Ù‡Ù…', 0.0), ('ÙØ±Ø¯Ø§', 0.0), ('Ø´Ø¯Ù†', 0.0), ('Ø§ÛŒÙ†', 0.0)]\n"
          ]
        }
      ],
      "source": [
        "# 2. find 10 nearest words from \"Ø¢Ø²Ø§Ø¯ÛŒ\"\n",
        "embedding_dict = {word: encoding for word, encoding in zip(words, one_hot_encoded)}\n",
        "\n",
        "word = \"Ø¢Ø²Ø§Ø¯ÛŒ\"\n",
        "k = 10\n",
        "nearest_words = find_k_nearest_neighbors(word, embedding_dict, k)\n",
        "print(nearest_words)\n",
        "\n",
        "word = \"Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±\"\n",
        "nearest_words = find_k_nearest_neighbors(word, embedding_dict, k)\n",
        "print(nearest_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each and every vector in the one hot encoding is orthogonal to each other. So the cosine similarity as well as distance between any two vectors are same. Thus it holds no relationship among them.\n",
        "That is why the nearest words found are the same. The cosine similarity of each pair of words equals 0."
      ],
      "metadata": {
        "id": "_yinx6QW8EfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just testing\n",
        "count = 0\n",
        "for value in embedding_dict['Ø¢Ø²Ø§Ø¯ÛŒ']:\n",
        "  if value != 0.0:\n",
        "    print(count, value)\n",
        "  count = count + 1\n",
        "\n",
        "count = 0\n",
        "for value in embedding_dict['Ø¨Ù‡Ø§Ø±Ù‡']:\n",
        "  if value != 0.0:\n",
        "    print(count, value)\n",
        "  count = count + 1\n",
        "\n",
        "count = 0\n",
        "for value in embedding_dict['Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±']:\n",
        "  if value != 0.0:\n",
        "    print(count, value)\n",
        "  count = count + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pasR5WChpOKV",
        "outputId": "f5257422-1900-4f7b-8f31-601c7bbc0c50"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1805 1.0\n",
            "7146 1.0\n",
            "28946 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FayPXc1mvm2"
      },
      "source": [
        "##### Describe advantages and disadvantages of one-hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9tAFn4brKrB"
      },
      "source": [
        "**Advantages:**\n",
        "\n",
        "1. **Simplicity:** One-hot encoding is a straightforward and simple method to represent categorical variables. It involves creating a binary vector where each element corresponds to a unique category, making it easy to understand and implement.\n",
        "\n",
        "2. **Retains categorical information:** One-hot encoding preserves the categorical nature of the variable. Each category is represented by a separate binary variable, allowing models to capture relationships and patterns specific to each category.\n",
        "\n",
        "3. **Compatibility with machine learning algorithms:** Many machine learning algorithms require numerical inputs. One-hot encoding converts categorical variables into a numeric format that can be readily used by these algorithms.\n",
        "\n",
        "4. **Avoids ordinality assumption:** One-hot encoding treats all categories as independent and does not impose any ordinal relationship between categories. This is useful when there is no inherent order or hierarchy among the categories.\n",
        "\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "1. **Dimensionality:** One-hot encoding expands the dimensionality of the feature space. If a categorical variable has a large number of unique categories, the resulting one-hot encoded representation can lead to a high-dimensional feature space, which may impact computational efficiency and model complexity.\n",
        "\n",
        "2. **Curse of dimensionality:** The increase in dimensionality due to one-hot encoding can lead to the curse of dimensionality. This refers to the problem where the number of features becomes large relative to the number of observations, which can result in sparse data, increased model complexity, and overfitting.\n",
        "\n",
        "3. **Redundancy:** One-hot encoding can introduce redundancy in the data representation. Since each category is represented by a separate binary variable, there is a perfect correlation between these variables. This redundancy can lead to multicollinearity issues in some models.\n",
        "\n",
        "4. **Handling new categories:** One-hot encoding requires defining the set of categories in advance. If new categories appear during testing or deployment, the one-hot encoding scheme may not handle them properly. This can be particularly problematic in real-world scenarios where new categories may emerge over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHeSYFUKw5gw"
      },
      "source": [
        "# 3. TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tweets = data['Text']\n",
        "print(Tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WUmrM9dsOMS",
        "outputId": "91ed4a55-8fc7-4f37-f6ee-7987b0f883b3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        Ø¨Ù†Ø´ÛŒÙ† ØªØ§ Ø´ÙˆØ¯ Ù†Ù‚Ø´ ÙØ§Ù„ Ù…Ø§ \\nÙ†Ù‚Ø´ Ù‡Ù…â€Œ ÙØ±Ø¯Ø§ Ø´Ø¯Ù†\\n#Ù…...\n",
            "1        @Tanasoli_Return @dr_moosavi Ø§ÛŒÙ† Ú¯ÙˆØ²Ùˆ Ø±Ùˆ Ú©ÛŒ Ú¯Ø±...\n",
            "2        @ghazaleghaffary Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ØŒ Ø¨Ø±Ø§ÛŒ Ù…Ù‡Ø³Ø§.\\n#OpIr...\n",
            "3        @_hidden_ocean Ù…Ø±Ú¯ Ø¨Ø± Ø¯ÛŒÚ©ØªØ§ØªÙˆØ± \\n#OpIran \\n#Ma...\n",
            "4        Ù†Ø°Ø§Ø±ÛŒÙ… Ø®ÙˆÙ†Ø´ÙˆÙ† Ù¾Ø§ÛŒÙ…Ø§Ù„ Ø´Ù‡.â€Œâ€Œ.â€Œâ€Œ.\\n#Mahsa_Amini #...\n",
            "                               ...                        \n",
            "19995    Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ±Ø§Ù† Ø¨Ø§Ù†Ùˆ #Mahsa_Amini      #MahsaAmini ...\n",
            "19996    @MohammadTehra16 @mimpedram Ø§Ø² Ø¨Ø³ Ø­Ø§Ø¬ Ø®Ø§Ù†Ù… Ø¯Ø±Ø§...\n",
            "19997    Ø¨Ù‡ Ø§ÙØªØ®Ø§Ø± Ø§Ø² Ø¨ÛŒÙ† Ø±ÙØªÙ† Ø¬Ù…Ù‡ÙˆØ±ÛŒ Ø§Ø³Ù„Ø§Ù…ÛŒğŸ™†â€â™‚ï¸ğŸ™†â€â™‚ï¸ğŸ™†â€â™‚...\n",
            "19998    Ù¾Ù†Ø¬Ø§Ù‡ Ùˆ Ø´ÛŒØ´ \\n\\n#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \\n#Mahsa_Amini \\n#...\n",
            "19999    Ø¯Ø± Ù…Ø­ÛŒØ· Ø·ÙˆÙØ§Ù†â€ŒØ²Ø§ÛŒ Ù…Ø§Ù‡Ø±Ø§Ù†Ù‡ Ø¯Ø± Ø¬Ù†Ú¯ Ø§Ø³Øª\\nÙ†Ø§Ø®Ø¯Ø§ÛŒ Ø§...\n",
            "Name: Text, Length: 20000, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. find the TF-IDF of all tweets.\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# # Create a document-term matrix\n",
        "# vectorizer = TfidfVectorizer()\n",
        "# document_term_matrix = vectorizer.fit_transform(Tweets)\n",
        "\n",
        "# # Calculate the TF-IDF values\n",
        "# tfidf_values = document_term_matrix.toarray()\n",
        "\n",
        "# # Normalize the TF-IDF values\n",
        "# normalized_tfidf = tfidf_values / np.linalg.norm(tfidf_values, axis=1, keepdims=True)\n",
        "\n",
        "# # Print the TF-IDF values for the first 10 tweets\n",
        "# for i in range(10):\n",
        "#     print(\"Tweet:\", Tweets[i])\n",
        "#     print(\"TF-IDF:\", normalized_tfidf[i])\n",
        "#     print()\n",
        "\n",
        "# Create a TfidfVectorizer object and fit it to the preprocessed corpus\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(words)\n",
        "\n",
        "# Transform the preprocessed corpus into a TF-IDF matrix\n",
        "tf_idf_matrix = vectorizer.transform(words)\n",
        "\n",
        "# Get list of feature names that correspond to the columns in the TF-IDF matrix\n",
        "print(\"Feature Names:\\n\", vectorizer.get_feature_names_out())\n",
        "\n",
        "# Print the resulting matrix\n",
        "print(\"TF-IDF Matrix:\\n\", tf_idf_matrix.toarray())\n",
        "# for i in tf_idf_matrix.toarray():\n",
        "#   for j in i:\n",
        "#     if (j != 0):\n",
        "#       print (j)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yFr8FMAsbpJ",
        "outputId": "5f7d4764-445f-49de-a2c0-7635b7fe6a8c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names:\n",
            " ['00' '0020115687' '00971562643674' ... 'Û¹Û¸' 'Û¹Û¹' 'ïºØ³Øª']\n",
            "TF-IDF Matrix:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. choose one tweets randomly.\n",
        "import random\n",
        "chosen_tweet = random.choice(Tweets)\n",
        "print(\"Chosen Tweet:\", chosen_tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUJVtLSeE_yH",
        "outputId": "5c1fee68-265d-4e94-88e8-5fafd873deaa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen Tweet: @AnonymousUK2022 #Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#Mahsa_Amini #oplran \n",
            "Ø§ÛŒØ±Ø§Ù†Ùˆ Ù¾Ø³ Ù…ÛŒÚ¯ÛŒØ±ÛŒÙ…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SJMju0Tiw9YA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2765e128-796b-4b13-c8ba-482d4b9a3e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-fb2fe9b1de3f>:16: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  cosine_similarity = dot_product / (norm_u * norm_v)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 Nearest Tweets:\n",
            "========================\n",
            "Ø¨Ø±Ø§ÛŒ Ø¹Ù…Ø±Ù…ÙˆÙ† Ú©Ù‡ Ø³Ø± Ø§ÛŒÙ†ØªØ±Ù†Øª Ùˆ Ø¯ÙˆØ± Ø²Ø¯Ù† ÙÛŒÙ„ØªØ± Ù‡Ø§ Ø­Ø±ÙˆÙ… Ø´Ø¯..\n",
            "#MahsaAmini \n",
            "#Mahsa_Amini \n",
            "#OpIran\n",
            "========================\n",
            "@Chandlershelby1 Ø¨Ø±Ø§ÛŒ\n",
            " #Mahsa_Amini \n",
            "#OpIran \n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ\n",
            "========================\n",
            "Ø§ÙˆÙ†Ø§ÛŒÛŒ Ú©Ù‡ Ø¯ÙˆØ³Øª Ù†Ø¯Ø§Ø±Ù… Ù¾ÛŒØ¬ Ø§ÛŒÙ†Ø³ØªØ§Ø´ÙˆÙ† Ø§Ø² Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø®Ø§Ø±Ø¬ Ø¨Ø´Ù‡ Ø¨Ø¬Ø§ÛŒ Ù†Ø§Ù„Ù‡ Ø¯Ø± Ø§ÛŒÙ† Ø²Ù…ÛŒÙ†Ù‡ØŒ ÙÙ‚Ø· Ú©Ø§ÙÛŒÙ‡ Ù¾Ø³Øª Ùˆ Ø§Ø³ØªÙˆØ±ÛŒ Ø­Ù…Ø§ÛŒØª Ø§Ø² Ù…Ø±Ø¯Ù… Ø¨Ø²Ø§Ø±Ù†ØŒ Ù‡Ù…ÛŒÙ†.\n",
            "#MahsaAmini\n",
            "#Mahsa_Amini\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ\n",
            "#OpIran\n",
            "========================\n",
            "Ø§ÛŒÙ†Ø§ÛŒÛŒ Ú©Ù‡ Ù…ÛŒÚ¯Ù† ÙÙ„Ø§Ù† Ø§Ø³ØªØ§Ù† Ùˆ ÙÙ„Ø§Ù† Ø¬Ø§ Ùˆ Ù…Ø±Ú©Ø² Ú©Ø´ÙˆØ± Ø®Ø¨Ø±ÛŒ Ù†ÛŒØ³Øª Ø§ÙˆÙ„ Ø¨Ø±Ù† Ø¨Ø¨ÛŒÙ†Ù† Ú†Ù‚Ø¯Ø± Ø¨Ú†Ù‡Ø§Ù…ÙˆÙ†Ùˆ Ø²Ø¯Ù† Ú©Ø´ØªÙ† Ø¨Ø¹Ø¯ Ø¨ÛŒØ§Ù† Ø§ÛŒÙ†Ùˆ Ø¨Ø®ÙˆØ±Ù†\n",
            "\n",
            "ØªÙØ±Ù‚Ù‡ Ø§ÙÚ©Ù† Ù‡Ø§ÛŒ Ø±Ùˆ Ø§Ø¹ØµØ§Ø¨ Ø³Ø§ÛŒØ¨Ø±ÛŒ\n",
            "\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#Mahsa_Amini\n",
            "========================\n",
            "@mamadporii Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡Ø§ Ø¨ÛŒØ¯Ø§Ø±Ù†\n",
            "Ø§ÛŒÙ†Ùˆ Û²Ûµ Ø±ÙˆØ²Ù‡ Ú©Ù‡ Ø¯Ø§Ø±ÛŒÙ… Ù…ÛŒØ¨ÛŒÙ†ÛŒÙ… Ùˆ Ù„Ù…Ø³ Ù…ÛŒÚ©Ù†ÛŒÙ…\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#Mahsa_Amini \n",
            "#OpIran\n",
            "========================\n",
            "Ù…Ø§ Ù‡Ù…Ù‡ Ù…Ù‡Ø³Ø§ Ù‡Ø³ØªÛŒÙ… Ø¨Ø¬Ù†Ú¯ ØªØ§ Ø¨Ø¬Ù†Ú¯ÛŒÙ…\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ\n",
            "#Mahsa_Amini \n",
            "#MahsaAmini\n",
            "========================\n",
            "@asemanhn @Cherii98 ÛŒÚ©\n",
            "Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "Ø¯Ø®ØªØ± Ø§ÛŒØ±Ø§Ù†ğŸ–¤ğŸ–¤\n",
            "#Mahsa_Amini\n",
            "#OpIran\n",
            "========================\n",
            "@alikarimi_ak8 Ø¨Ø±Ø§ÛŒ Ø§Ù…ÛŒØ¯ Ø¨Ù‡ Ø¢ÛŒÙ†Ø¯Ù‡ Ø§ÛŒ Ø±ÙˆØ´Ù† Ø¨Ø±Ø§ÛŒ ÙØ±Ø²Ù†Ø¯Ø§Ù† Ø§ÛŒØ±Ø§Ù† Ø²Ù…ÛŒÙ† #Mahsa_Amini\n",
            "========================\n",
            "@OutFarsi @armin_prm #OpIran \n",
            "#Mahsa_Amini \n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "Ù…Ø§ Ù‡Ù…Ù‡ Ø¨Ø§Ù‡Ù… Ù‡Ø³ØªÛŒÙ…\n",
            "========================\n",
            "@Nafise1375 @Godofpersiian Ø®ÛŒÙ„ÛŒØ§ Ø¯Ø§Ø±Ù† Ù‡Ø´ØªÚ¯ Ø§Ø´ØªØ¨Ø§Ù‡ Ù…ÛŒØ²Ù†Ù†ØŒ Ø³Ø§ÛŒØ¨Ø±ÛŒ Ù‡Ù… Ù†ÛŒØ³ØªÙ†Â» Ù„Ø·ÙÙ† Ø¢Ú¯Ø§Ù‡Ø´ÙˆÙ† Ú©Ù†ÛŒÙ†...\n",
            "Ø¯Ù‚Øª Ú©Ù†ÛŒØ¯ #Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ Ø¯Ø±Ø³ØªÙ‡ ÛŒÙ‡ Ø¢Ù†Ø¯Ø±Ù„Ø§ÛŒÙ† Ø¨ÛŒØ´ØªØ± Ù†Ø¯Ø§Ø±Ù‡.#Ø§Ø¹ØªØµØ§Ø¨Ø§Øª_Ø³Ø±Ø§Ø±ÛŒ \n",
            "Ù‡Ø´ØªÚ¯ Ø±Ùˆ Ø®ÙˆØ¯ØªÙˆÙ† Ø¨Ù†ÙˆÛŒØ³ÛŒÙ†ØŒ Ø§Ø² Ø§Ù†ØªØ®Ø§Ø¨Ù‡Ø§ÛŒÛŒ Ú©Ù‡ ØªÙˆÛŒÛŒØªØ± Ø¨Ù‡ØªÙˆÙ† Ù…ÛŒØ¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒÙ†...\n",
            " #MashaAmini \n",
            "#OpIran\n",
            "#Mahsa_Amini\n"
          ]
        }
      ],
      "source": [
        "# 3. find 10 nearest tweets from chosen tweet.\n",
        "\n",
        "# Get the index of the chosen tweet in the tfidf_matrix\n",
        "chosen_tweet_index = np.where(Tweets == chosen_tweet)[0][0]\n",
        "\n",
        "# Get the TF-IDF vector for the chosen tweet\n",
        "chosen_tweet_vector = tf_idf_matrix[chosen_tweet_index]\n",
        "\n",
        "# Calculate the cosine similarity between the chosen tweet and all other tweets\n",
        "similarities = []\n",
        "for i in range(len(Tweets)):\n",
        "    similarity = cosine_similarity(chosen_tweet_vector.toarray()[0], tf_idf_matrix[i].toarray()[0])\n",
        "    similarities.append(similarity)\n",
        "\n",
        "# Sort the similarities and get the indices of the 10 nearest tweets\n",
        "nearest_indices = np.argsort(similarities)[::-1][1:11]\n",
        "\n",
        "print(\"10 Nearest Tweets:\")\n",
        "for index in nearest_indices:\n",
        "    print(\"========================\")\n",
        "    print(Tweets[index])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chosen_tweet = random.choice(Tweets)\n",
        "print(\"Chosen Tweet:\", chosen_tweet)\n",
        "\n",
        "# Get the index of the chosen tweet in the tfidf_matrix\n",
        "chosen_tweet_index = np.where(Tweets == chosen_tweet)[0][0]\n",
        "\n",
        "# Get the TF-IDF vector for the chosen tweet\n",
        "chosen_tweet_vector = tf_idf_matrix[chosen_tweet_index]\n",
        "\n",
        "# Calculate the cosine similarity between the chosen tweet and all other tweets\n",
        "similarities = []\n",
        "for i in range(len(Tweets)):\n",
        "    similarity = cosine_similarity(chosen_tweet_vector.toarray()[0], tf_idf_matrix[i].toarray()[0])\n",
        "    similarities.append(similarity)\n",
        "\n",
        "# Sort the similarities and get the indices of the 10 nearest tweets\n",
        "nearest_indices = np.argsort(similarities)[::-1][1:11]\n",
        "\n",
        "print(\"10 Nearest Tweets:\")\n",
        "for index in nearest_indices:\n",
        "    print(\"========================\")\n",
        "    print(Tweets[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq1wZeHdFjU3",
        "outputId": "0a8d0634-4b53-46ff-82dc-0001d56ebcd1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen Tweet: @Delaramm1127 Ú©ÛŒÙ… ØªÙ‡ÛŒÙˆÙ†Ú¯ \n",
            "#Mahsa_Amini\n",
            "#OpIran\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-fb2fe9b1de3f>:16: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  cosine_similarity = dot_product / (norm_u * norm_v)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 Nearest Tweets:\n",
            "========================\n",
            "@Nafise1375 @Godofpersiian Ø®ÛŒÙ„ÛŒØ§ Ø¯Ø§Ø±Ù† Ù‡Ø´ØªÚ¯ Ø§Ø´ØªØ¨Ø§Ù‡ Ù…ÛŒØ²Ù†Ù†ØŒ Ø³Ø§ÛŒØ¨Ø±ÛŒ Ù‡Ù… Ù†ÛŒØ³ØªÙ†Â» Ù„Ø·ÙÙ† Ø¢Ú¯Ø§Ù‡Ø´ÙˆÙ† Ú©Ù†ÛŒÙ†...\n",
            "Ø¯Ù‚Øª Ú©Ù†ÛŒØ¯ #Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ Ø¯Ø±Ø³ØªÙ‡ ÛŒÙ‡ Ø¢Ù†Ø¯Ø±Ù„Ø§ÛŒÙ† Ø¨ÛŒØ´ØªØ± Ù†Ø¯Ø§Ø±Ù‡.#Ø§Ø¹ØªØµØ§Ø¨Ø§Øª_Ø³Ø±Ø§Ø±ÛŒ \n",
            "Ù‡Ø´ØªÚ¯ Ø±Ùˆ Ø®ÙˆØ¯ØªÙˆÙ† Ø¨Ù†ÙˆÛŒØ³ÛŒÙ†ØŒ Ø§Ø² Ø§Ù†ØªØ®Ø§Ø¨Ù‡Ø§ÛŒÛŒ Ú©Ù‡ ØªÙˆÛŒÛŒØªØ± Ø¨Ù‡ØªÙˆÙ† Ù…ÛŒØ¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ù†ÛŒÙ†...\n",
            " #MashaAmini \n",
            "#OpIran\n",
            "#Mahsa_Amini\n",
            "========================\n",
            "Ø¨Ù‡ Ø®Ø§Ø·Ø± Ù‡Ù…Ù‡ Ø´Ù‡ÛŒØ¯Ø§ÛŒÛŒ Ú©Ù‡ Ø§ÛŒÙ† Ú†Ù†Ø¯ Ø±ÙˆØ² Ùˆ ØªÙ…Ø§Ù… Ø§ÛŒÙ† Ø³Ø§Ù„Ù‡Ø§ Ø¯Ø§Ø¯ÛŒÙ…...\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#Mahsa_Amini \n",
            "#OpIran\n",
            "========================\n",
            "@Chandlershelby1 Ø¨Ø±Ø§ÛŒ\n",
            " #Mahsa_Amini \n",
            "#OpIran \n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ\n",
            "========================\n",
            "Ø§ÙˆÙ†Ø§ÛŒÛŒ Ú©Ù‡ Ø¯ÙˆØ³Øª Ù†Ø¯Ø§Ø±Ù… Ù¾ÛŒØ¬ Ø§ÛŒÙ†Ø³ØªØ§Ø´ÙˆÙ† Ø§Ø² Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø®Ø§Ø±Ø¬ Ø¨Ø´Ù‡ Ø¨Ø¬Ø§ÛŒ Ù†Ø§Ù„Ù‡ Ø¯Ø± Ø§ÛŒÙ† Ø²Ù…ÛŒÙ†Ù‡ØŒ ÙÙ‚Ø· Ú©Ø§ÙÛŒÙ‡ Ù¾Ø³Øª Ùˆ Ø§Ø³ØªÙˆØ±ÛŒ Ø­Ù…Ø§ÛŒØª Ø§Ø² Ù…Ø±Ø¯Ù… Ø¨Ø²Ø§Ø±Ù†ØŒ Ù‡Ù…ÛŒÙ†.\n",
            "#MahsaAmini\n",
            "#Mahsa_Amini\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ\n",
            "#OpIran\n",
            "========================\n",
            "Ø§ÛŒÙ†Ø§ÛŒÛŒ Ú©Ù‡ Ù…ÛŒÚ¯Ù† ÙÙ„Ø§Ù† Ø§Ø³ØªØ§Ù† Ùˆ ÙÙ„Ø§Ù† Ø¬Ø§ Ùˆ Ù…Ø±Ú©Ø² Ú©Ø´ÙˆØ± Ø®Ø¨Ø±ÛŒ Ù†ÛŒØ³Øª Ø§ÙˆÙ„ Ø¨Ø±Ù† Ø¨Ø¨ÛŒÙ†Ù† Ú†Ù‚Ø¯Ø± Ø¨Ú†Ù‡Ø§Ù…ÙˆÙ†Ùˆ Ø²Ø¯Ù† Ú©Ø´ØªÙ† Ø¨Ø¹Ø¯ Ø¨ÛŒØ§Ù† Ø§ÛŒÙ†Ùˆ Ø¨Ø®ÙˆØ±Ù†\n",
            "\n",
            "ØªÙØ±Ù‚Ù‡ Ø§ÙÚ©Ù† Ù‡Ø§ÛŒ Ø±Ùˆ Ø§Ø¹ØµØ§Ø¨ Ø³Ø§ÛŒØ¨Ø±ÛŒ\n",
            "\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#Mahsa_Amini\n",
            "========================\n",
            "@mamadporii Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡Ø§ Ø¨ÛŒØ¯Ø§Ø±Ù†\n",
            "Ø§ÛŒÙ†Ùˆ Û²Ûµ Ø±ÙˆØ²Ù‡ Ú©Ù‡ Ø¯Ø§Ø±ÛŒÙ… Ù…ÛŒØ¨ÛŒÙ†ÛŒÙ… Ùˆ Ù„Ù…Ø³ Ù…ÛŒÚ©Ù†ÛŒÙ…\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "#Mahsa_Amini \n",
            "#OpIran\n",
            "========================\n",
            "Ù…Ø§ Ù‡Ù…Ù‡ Ù…Ù‡Ø³Ø§ Ù‡Ø³ØªÛŒÙ… Ø¨Ø¬Ù†Ú¯ ØªØ§ Ø¨Ø¬Ù†Ú¯ÛŒÙ…\n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ\n",
            "#Mahsa_Amini \n",
            "#MahsaAmini\n",
            "========================\n",
            "@asemanhn @Cherii98 ÛŒÚ©\n",
            "Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "Ø¯Ø®ØªØ± Ø§ÛŒØ±Ø§Ù†ğŸ–¤ğŸ–¤\n",
            "#Mahsa_Amini\n",
            "#OpIran\n",
            "========================\n",
            "@OutFarsi @armin_prm #OpIran \n",
            "#Mahsa_Amini \n",
            "#Ù…Ù‡Ø³Ø§_Ø§Ù…ÛŒÙ†ÛŒ \n",
            "Ù…Ø§ Ù‡Ù…Ù‡ Ø¨Ø§Ù‡Ù… Ù‡Ø³ØªÛŒÙ…\n",
            "========================\n",
            "@alikarimi_ak8 Ø¨Ø±Ø§ÛŒ Ø§Ù…ÛŒØ¯ Ø¨Ù‡ Ø¢ÛŒÙ†Ø¯Ù‡ Ø§ÛŒ Ø±ÙˆØ´Ù† Ø¨Ø±Ø§ÛŒ ÙØ±Ø²Ù†Ø¯Ø§Ù† Ø§ÛŒØ±Ø§Ù† Ø²Ù…ÛŒÙ† #Mahsa_Amini\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqNCL4kFmvm3"
      },
      "source": [
        "##### Describe advantages and disadvantages of TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GHvpTAZu7ZU"
      },
      "source": [
        "**Advantages:**\n",
        "1. **Term Importance:** TF-IDF highlights important terms in a document by assigning higher weights to words that are more frequent in the document and less frequent in the entire corpus. This allows for effective keyword extraction and helps in identifying the most relevant terms within a document.\n",
        "\n",
        "2. **Document Similarity:** TF-IDF enables the calculation of cosine similarity between documents based on their TF-IDF vector representations. This similarity measure is useful for tasks such as document clustering, information retrieval, and recommendation systems.\n",
        "\n",
        "3. **Language Independence:** TF-IDF is language-independent, meaning it can be applied to documents in any language. It doesn't rely on language-specific rules or heuristics, making it a versatile technique for text analysis across different languages.\n",
        "\n",
        "4. **Computational Efficiency:** TF-IDF can be computed efficiently, especially when using sparse matrix representations. This makes it scalable for large corpora and enables fast retrieval of relevant documents based on query terms.\n",
        "\n",
        "**Disadvantages:**\n",
        "1. **Term Frequency Bias:** TF-IDF heavily relies on term frequency. Overly frequent terms within a document may dominate the TF-IDF score, potentially overshadowing other important terms. This can be mitigated by using term frequency normalization techniques.\n",
        "2. **Lack of Semantic Understanding:** TF-IDF does not capture the semantic meaning of words or the relationships between them. It treats each term independently, which may limit its ability to capture the context or nuanced meaning of phrases or multi-word expressions.\n",
        "3. **Handling Out-of-Vocabulary Words:** TF-IDF is based on a fixed vocabulary derived from the corpus. Out-of-vocabulary words, i.e., words not present in the vocabulary, are typically ignored or treated as noise. This can be a limitation when dealing with specialized or domain-specific terms.\n",
        "4. **Document Length Bias:** Longer documents tend to have higher term frequencies, which can bias the TF-IDF scores. Longer documents may have higher TF-IDF values simply due to more occurrences of terms, even if the terms are not necessarily more important.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INM6vtm2zqJs"
      },
      "source": [
        "# 4. Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCnxqaVY2zCc"
      },
      "outputs": [],
      "source": [
        "# 1. train a word2vec model base on all tweets\n",
        "# 2. find 10 nearest words from \"Ø¢Ø²Ø§Ø¯ÛŒ\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37oMUvZcmvm4"
      },
      "source": [
        "##### Describe advantages and disadvantages of Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv39LXf0wkjd"
      },
      "source": [
        "Advantages:\n",
        "\n",
        "\n",
        "Disadvantages:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSdlWMl64aPN"
      },
      "source": [
        "# 5. Contextualized embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgVAEQhyOPxg"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfKEqNml6eEB"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc8hBCnn4cV_"
      },
      "outputs": [],
      "source": [
        "# 1. fine-tune the model base on all tweets\n",
        "# 2. find 10 nearest words from \"Ø¢Ø²Ø§Ø¯ÛŒ\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBUkqdX0mvm6"
      },
      "source": [
        "##### Describe advantages and disadvantages of Contextualized embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al-4HT8KxTwd"
      },
      "source": [
        "Advantages:\n",
        "\n",
        "\n",
        "Disadvantages:\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}