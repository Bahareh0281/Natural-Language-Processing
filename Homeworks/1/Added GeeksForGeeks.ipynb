{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Q1: Probabilistic N-Gram Language Model(50 points)"
      ],
      "metadata": {
        "id": "a4NQTign_k_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective:**\n",
        "\n",
        "The objective of this question is to implement and experiment with an N-Gram language model using the Reuters dataset. The task involves building a probabilistic N-Gram model and creating a text generator based on the trained model with customizable parameters.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "\n",
        "**1.Text Preprocessing (5 points):**\n",
        "*   Implement the preprocess_text function to perform necessary text preprocessing. You may use NLTK or other relevant libraries for this task. (Already provided, no modification needed)\n",
        "\n",
        "\n",
        "**2.Build Probabilistic N-Gram Model (15 points):**\n",
        "\n",
        "*   Implement the build_probabilistic_ngram_model function to construct a probabilistic N-Gram model from the Reuters dataset.\n",
        "\n",
        "\n",
        "**3.Generate Text with Customizable Parameters (15 points):**\n",
        "\n",
        "*   Implement the generate_text function to generate text given a seed text and the probabilistic N-Gram model.\n",
        "*   The function should have parameters for probability_threshold and min_length to customize the generation process.\n",
        "*   Ensure that the generation stops when either the specified min_length is reached or the probabilities fall below probability_threshold.\n",
        "\n",
        "\n",
        "**4.Experimentation and Parameter Tuning (5 points):**\n",
        "\n",
        "*   Use Google Colab to experiment with different values of n_value, probability_threshold, and min_length.\n",
        "Find the optimal parameters that result in coherent and meaningful generated text.\n",
        "*   Provide a detailed analysis of the impact of changing each parameter on the generated text's quality.\n",
        "*   Discuss any challenges faced during parameter tuning and propose potential improvements.\n",
        "\n",
        "\n",
        "**5.Results and Conclusion (10 points):**\n",
        "\n",
        "*   Summarize your findings and present the optimal parameter values for n_value, probability_threshold, and min_length.\n",
        "*   Discuss the trade-offs and considerations when selecting these parameters.\n",
        "*   Conclude with insights gained from the experimentation."
      ],
      "metadata": {
        "id": "zDKtnG-HAH1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "from nltk import ngrams\n",
        "import random\n",
        "import string\n",
        "from collections import defaultdict\n",
        "\n",
        "# Download the Reuters dataset if not already downloaded\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "3NWXJy-T-Vd7",
        "outputId": "ecd2c1af-03ec-4010-9ba7-4ad6c14795ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9IHxAbU0N80"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    string.punctuation = string.punctuation +'\"'+'\"'+'-'+'''+'''+'—'\n",
        "    removal_list = list(stop_words) + list(string.punctuation) + ['lt', 'rt']\n",
        "\n",
        "    processed_text = []\n",
        "    for sentence in text:\n",
        "        sentence = list(map(lambda x: x.lower(), sentence))\n",
        "        for word in sentence:\n",
        "            if word == '.':\n",
        "                sentence.remove(word)\n",
        "            else:\n",
        "                processed_text.append(word)\n",
        "    return processed_text\n",
        "\n",
        "def build_probabilistic_ngram_model(corpus, n):\n",
        "    ngram = []\n",
        "    for sentence in corpus:\n",
        "        tokenized_sentence = list(ngrams(sentence, n, pad_left=True, pad_right=True))\n",
        "        ngram.extend(tokenized_sentence)\n",
        "\n",
        "    model = {}\n",
        "    for tokens in ngram:\n",
        "        prefix = tokens[:n-1]\n",
        "        next_word = tokens[n-1]\n",
        "        if prefix not in model:\n",
        "            model[prefix] = Counter()\n",
        "        model[prefix][next_word] += 1\n",
        "\n",
        "    return model\n",
        "\n",
        "def generate_text(model, seed_text, n, probability_threshold=0.1, min_length=10):\n",
        "    prefix = tuple(seed_text.split()[-(n-1):])\n",
        "    generated_text = list(seed_text.split())\n",
        "\n",
        "    while len(generated_text) < min_length or (generated_text[-1] != '.' and generated_text[-1] != '.\\''):\n",
        "        if prefix in model:\n",
        "            candidates = model[prefix]\n",
        "            total_count = sum(candidates.values())\n",
        "            probabilities = {word: count/total_count for word, count in candidates.items()}\n",
        "            next_word = random.choices(list(probabilities.keys()), list(probabilities.values()))[0]\n",
        "\n",
        "            if probabilities[next_word] >= probability_threshold:\n",
        "                generated_text.append(next_word)\n",
        "                prefix = tuple(generated_text[-(n-1):])\n",
        "            else:\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return ' '.join(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the code\n",
        "sents = reuters.sents()\n",
        "processed_text = preprocess_text(sents)\n",
        "model = build_probabilistic_ngram_model(processed_text, 3)\n",
        "seed_text = 'He said'\n",
        "generated_text = generate_text(model, seed_text, 3, probability_threshold=0.1, min_length=10)\n",
        "\n",
        "print(f'Seed Text: {seed_text}')\n",
        "print(f'Generated Text: {generated_text}')\n",
        "\n",
        "# Next word prediction\n",
        "s=''\n",
        "def pick_word(counter):\n",
        "\t\"Chooses a random element.\"\n",
        "\treturn random.choice(list(counter.elements()))\n",
        "prefix = \"he\", \"said\"\n",
        "print(\" \".join(prefix))\n",
        "s = \" \".join(prefix)\n",
        "for i in range(19):\n",
        "\tsuffix = pick_word(d[prefix])\n",
        "\ts=s+' '+suffix\n",
        "\tprint(s)\n",
        "\tprefix = prefix[1], suffix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "v1Jj_6ekhUx9",
        "outputId": "5434a50f-274a-445e-a310-d06149e119df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed Text: He said\n",
            "Generated Text: He said\n",
            "he said\n",
            "he said ,\n",
            "he said , a\n",
            "he said , a major\n",
            "he said , a major turnaround\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "('major', 'turnaround')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-501cf14cafe3>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpick_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ('major', 'turnaround')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Reuters dataset\n",
        "corpus = [reuters.raw(file_id) for file_id in reuters.fileids()]\n",
        "\n",
        "# Preprocess the entire corpus\n",
        "preprocessed_corpus = [preprocess_text(text) for text in corpus]\n",
        "\n",
        "# Choose an n for the n-gram model\n",
        "n_value = 2  # You may change this value\n",
        "\n",
        "# Build the probabilistic n-gram model\n",
        "probabilistic_ngram_model = build_probabilistic_ngram_model(preprocessed_corpus, n_value)"
      ],
      "metadata": {
        "id": "eVVMe_s59Ngd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the text generator\n",
        "seed_text = \"Inflation is\"\n",
        "generated_text = generate_text(probabilistic_ngram_model, seed_text, n_value, probability_threshold=0.02, min_length=5)\n",
        "print(f\"Generated Text: {generated_text}\")"
      ],
      "metadata": {
        "id": "n-4WP7IC9Q7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05e51e2-aa7e-49c3-96ee-1972ea126b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: inflation is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Reuters dataset\n",
        "corpus = [reuters.raw(file_id) for file_id in reuters.fileids()]\n",
        "\n",
        "# Preprocess the entire corpus\n",
        "preprocessed_corpus = [preprocess_text(text) for text in corpus]\n",
        "\n",
        "# Test the text generator\n",
        "seed_text = \"Inflation is\"\n",
        "\n",
        "for n_value in range (2, 4):\n",
        "  print(\"n is: \", n_value)\n",
        "  # Build the probabilistic n-gram model\n",
        "  probabilistic_ngram_model = build_probabilistic_ngram_model(preprocessed_corpus, n_value)\n",
        "  generated_text = generate_text(probabilistic_ngram_model, seed_text, n_value, probability_threshold=0.02, min_length=10)\n",
        "  print(f\"Generated Text: {generated_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "n_NV9OZiQZTc",
        "outputId": "cb2d2ba9-76bc-4c99-d680-b30ac2e69a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-66e59abbe047>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Preprocess the entire corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpreprocessed_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Test the text generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-66e59abbe047>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Preprocess the entire corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpreprocessed_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Test the text generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-5080b797010b>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprocessed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import string\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('reuters')\n",
        "from nltk.corpus import reuters\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import stopwords  # Add this line to import the stopwords module\n",
        "from collections import Counter  # Add this line to import the Counter class\n",
        "\n",
        "# input the reuters sentences\n",
        "sents =reuters.sents()\n",
        "\n",
        "# write the removal characters such as : Stopwords and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "string.punctuation = string.punctuation +'\"'+'\"'+'-'+'''+'''+'—'\n",
        "string.punctuation\n",
        "removal_list = list(stop_words) + list(string.punctuation)+ ['lt','rt']\n",
        "removal_list\n",
        "\n",
        "# generate unigrams bigrams trigrams\n",
        "unigram=[]\n",
        "bigram=[]\n",
        "trigram=[]\n",
        "tokenized_text=[]\n",
        "for sentence in sents:\n",
        "  sentence = list(map(lambda x:x.lower(),sentence))\n",
        "  for word in sentence:\n",
        "      if word== '.':\n",
        "        sentence.remove(word)\n",
        "      else:\n",
        "        unigram.append(word)\n",
        "\n",
        "  tokenized_text.append(sentence)\n",
        "  bigram.extend(list(ngrams(sentence, 2,pad_left=True, pad_right=True)))\n",
        "  trigram.extend(list(ngrams(sentence, 3, pad_left=True, pad_right=True)))\n",
        "\n",
        "# remove the n-grams with removable words\n",
        "def remove_stopwords(x):\n",
        "\ty = []\n",
        "\tfor pair in x:\n",
        "\t\tcount = 0\n",
        "\t\tfor word in pair:\n",
        "\t\t\tif word in removal_list:\n",
        "\t\t\t\tcount = count or 0\n",
        "\t\t\telse:\n",
        "\t\t\t\tcount = count or 1\n",
        "\t\tif (count==1):\n",
        "\t\t\ty.append(pair)\n",
        "\treturn (y)\n",
        "unigram = remove_stopwords(unigram)\n",
        "bigram = remove_stopwords(bigram)\n",
        "trigram = remove_stopwords(trigram)\n",
        "\n",
        "# generate frequency of n-grams\n",
        "freq_bi = FreqDist(bigram)\n",
        "freq_tri = FreqDist(trigram)\n",
        "\n",
        "d = {}\n",
        "for a, b, c in freq_tri:\n",
        "    if a is not None and b is not None and c is not None:\n",
        "        if (a, b) not in d:\n",
        "            d[a, b] = Counter()\n",
        "        d[a, b] += Counter({c: freq_tri[a, b, c]})\n",
        "\n",
        "\n",
        "# Next word prediction\n",
        "s=''\n",
        "def pick_word(counter):\n",
        "\t\"Chooses a random element.\"\n",
        "\treturn random.choice(list(counter.elements()))\n",
        "prefix = \"he\", \"said\"\n",
        "print(\" \".join(prefix))\n",
        "s = \" \".join(prefix)\n",
        "for i in range(19):\n",
        "\tsuffix = pick_word(d[prefix])\n",
        "\ts=s+' '+suffix\n",
        "\tprint(s)\n",
        "\tprefix = prefix[1], suffix\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y53qVVuMbOON",
        "outputId": "5278cc1a-155a-46cf-d183-fd9e1a2be0ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he said\n",
            "he said citibank\n",
            "he said citibank n\n",
            "he said citibank n a\n",
            "he said citibank n a .-\n",
            "he said citibank n a .- c\n",
            "he said citibank n a .- c 9\n",
            "he said citibank n a .- c 9 5\n",
            "he said citibank n a .- c 9 5 pct\n",
            "he said citibank n a .- c 9 5 pct to\n",
            "he said citibank n a .- c 9 5 pct to 2\n",
            "he said citibank n a .- c 9 5 pct to 2 ,\n",
            "he said citibank n a .- c 9 5 pct to 2 , after\n",
            "he said citibank n a .- c 9 5 pct to 2 , after trading\n",
            "he said citibank n a .- c 9 5 pct to 2 , after trading late\n",
            "he said citibank n a .- c 9 5 pct to 2 , after trading late yesterday\n",
            "he said citibank n a .- c 9 5 pct to 2 , after trading late yesterday that\n",
            "he said citibank n a .- c 9 5 pct to 2 , after trading late yesterday that grain\n",
            "he said citibank n a .- c 9 5 pct to 2 , after trading late yesterday that grain prices\n",
            "he said citibank n a .- c 9 5 pct to 2 , after trading late yesterday that grain prices would\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q2: Sentiment Analysis with Naive Bayes Classifier(50 Points)"
      ],
      "metadata": {
        "id": "dZ3XzDx7JUNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective:**\n",
        "\n",
        "You are tasked with implementing a Naive Bayes classifier for sentiment analysis. The provided code is incomplete, and your goal is to complete the missing parts. Additionally, you should train the classifier on a small dataset and analyze its performance.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1.**Complete the Code (35 points)**: Fill in the missing parts in the provided Python code for the Naive Bayes classifier. Pay special attention to the `extract_features` function.\n",
        "\n",
        "2.**Train and Test**: Train the Naive Bayes classifier on the training data and test it on a separate test set. Evaluate the accuracy of the classifier.\n",
        "\n",
        "3.**Analysis (15 points)**: Discuss the results. Identify any misclassifications and try to understand why the classifier may fail in those cases. Provide examples of sentences that were not predicted correctly and explain possible reasons.\n"
      ],
      "metadata": {
        "id": "NMuVkjW2XfAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "import string\n",
        "from collections import defaultdict\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import movie_reviews\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "M68XJubdKeDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(tokens):\n",
        "    # Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Perform stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "KSLo4_JoUcax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBayesClassifier:\n",
        "    def __init__(self, classes):\n",
        "        self.classes = classes\n",
        "        self.class_probs = defaultdict(float)\n",
        "        self.feature_probs = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    def train(self, training_data):\n",
        "        # Implement training here\n",
        "        # You should use get_features function to extract useful tokens from\n",
        "        # dataset and use them to train the classifier.\n",
        "        pass\n",
        "\n",
        "    def classify(self, features):\n",
        "        # Implement classification here\n",
        "        pass"
      ],
      "metadata": {
        "id": "m2Whvjy_Jq8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the movie reviews dataset from NLTK\n",
        "data = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "random.shuffle(data)\n",
        "\n",
        "# Shuffle the dataset for randomness\n",
        "random.shuffle(data)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(data) * split_ratio)\n",
        "train_set = data[:split_index]\n",
        "test_set = data[split_index:]\n",
        "\n",
        "# Train the Naive Bayes classifier\n",
        "classes = set(sentiment for _, sentiment in train_set)\n",
        "classifier = NaiveBayesClassifier(classes)\n",
        "classifier.train(train_set)\n",
        "\n",
        "def calculate_accuracy(dataset, dataset_type):\n",
        "    # Test the classifier on the testing set\n",
        "    correct_predictions = 0\n",
        "    for example in dataset:\n",
        "        tokens, true_sentiment = example\n",
        "        features = get_features(tokens)\n",
        "        predicted_sentiment = classifier.classify(features)\n",
        "        if predicted_sentiment == true_sentiment:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / len(dataset)\n",
        "    print(f\"{dataset_type} Accuracy: {accuracy}\")\n",
        "\n",
        "calculate_accuracy(train_set, 'Train')\n",
        "calculate_accuracy(test_set, 'Test')"
      ],
      "metadata": {
        "id": "j2jeyI6nKooE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Submission Instructions:\n"
      ],
      "metadata": {
        "id": "Nfl8UA42Gqjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Submit a Google Colab notebook containing your completed code and experimentation results.\n",
        "\n",
        "2.Include comments and explanations in your code to help understand the implemented logic.\n",
        "\n",
        "3.Clearly present the results of your parameter tuning in the notebook.\n",
        "\n",
        "4.Provide a brief summary of your findings and insights in the conclusion section.\n",
        "\n",
        "**Additional Notes:**\n",
        "*   Ensure that the notebook runs successfully in Google Colab.\n",
        "*   Experiment with various seed texts to showcase the diversity of generated text.\n",
        "*   Document any issues encountered during experimentation and how you addressed them.\n",
        "\n",
        "**Grading:**\n",
        "*   Each task will be graded out of the specified points.\n",
        "*   Points will be awarded for correctness, clarity of code, thorough experimentation, and insightful analysis."
      ],
      "metadata": {
        "id": "75kVTQX6GsCn"
      }
    }
  ]
}